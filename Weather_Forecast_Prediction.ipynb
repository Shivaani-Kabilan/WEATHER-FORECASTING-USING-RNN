{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Chennai_avg_temp (3).csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month_No.</th>\n",
       "      <th>Day</th>\n",
       "      <th>Year</th>\n",
       "      <th>Avg. Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1995</td>\n",
       "      <td>71.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1995</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1995</td>\n",
       "      <td>70.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1995</td>\n",
       "      <td>69.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1995</td>\n",
       "      <td>71.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month_No.  Day  Year  Avg. Temp\n",
       "0          1    1  1995       71.8\n",
       "1          1    2  1995       72.0\n",
       "2          1    3  1995       70.3\n",
       "3          1    4  1995       69.7\n",
       "4          1    5  1995       71.3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "displaying some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mod1 = dataset[['Avg. Temp']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Avg. Temp\n",
       "0       71.8\n",
       "1       72.0\n",
       "2       70.3\n",
       "3       69.7\n",
       "4       71.3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_mod1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      71.8\n",
       "1      72.0\n",
       "2      70.3\n",
       "3      69.7\n",
       "4      71.3\n",
       "       ... \n",
       "360    77.4\n",
       "361    78.1\n",
       "362    75.6\n",
       "363    76.2\n",
       "364    75.9\n",
       "Name: Avg. Temp, Length: 365, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set = dataset_mod1.iloc[0:365,0]\n",
    "\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Scaling \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = training_set.values.reshape(len(training_set), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 71.8],\n",
       "       [ 72. ],\n",
       "       [ 70.3],\n",
       "       [ 69.7],\n",
       "       [ 71.3],\n",
       "       [ 73.9],\n",
       "       [ 72.9],\n",
       "       [ 70.2],\n",
       "       [ 68.4],\n",
       "       [ 68. ],\n",
       "       [ 69.9],\n",
       "       [ 69.8],\n",
       "       [ 71. ],\n",
       "       [ 73.9],\n",
       "       [ 73.4],\n",
       "       [ 70.4],\n",
       "       [ 74.9],\n",
       "       [ 74.5],\n",
       "       [ 73. ],\n",
       "       [ 71.9],\n",
       "       [ 71.3],\n",
       "       [ 73.3],\n",
       "       [ 71.4],\n",
       "       [ 71.3],\n",
       "       [ 70.2],\n",
       "       [ 73. ],\n",
       "       [ 73.8],\n",
       "       [ 77.5],\n",
       "       [ 76.8],\n",
       "       [ 76.1],\n",
       "       [ 78.3],\n",
       "       [ 78.9],\n",
       "       [ 78.3],\n",
       "       [ 73.3],\n",
       "       [ 74.7],\n",
       "       [ 77.3],\n",
       "       [ 80.2],\n",
       "       [ 83.2],\n",
       "       [ 81. ],\n",
       "       [ 78.8],\n",
       "       [ 77.8],\n",
       "       [ 76.8],\n",
       "       [ 75.9],\n",
       "       [ 75.7],\n",
       "       [ 74.1],\n",
       "       [ 73.4],\n",
       "       [ 74.5],\n",
       "       [ 75.3],\n",
       "       [ 72.3],\n",
       "       [ 73.1],\n",
       "       [ 76.9],\n",
       "       [ 77.9],\n",
       "       [ 74.6],\n",
       "       [ 76.2],\n",
       "       [ 72.3],\n",
       "       [ 76.1],\n",
       "       [ 76.2],\n",
       "       [ 75.3],\n",
       "       [ 73.9],\n",
       "       [ 74.2],\n",
       "       [ 77.1],\n",
       "       [ 79.6],\n",
       "       [ 78.8],\n",
       "       [ 78.7],\n",
       "       [ 77.5],\n",
       "       [ 77. ],\n",
       "       [ 76.5],\n",
       "       [ 76. ],\n",
       "       [ 78. ],\n",
       "       [ 78.1],\n",
       "       [ 81.2],\n",
       "       [ 78.8],\n",
       "       [ 79.8],\n",
       "       [ 79.9],\n",
       "       [ 80.7],\n",
       "       [ 78.7],\n",
       "       [ 77.6],\n",
       "       [ 79.2],\n",
       "       [ 81. ],\n",
       "       [ 86.5],\n",
       "       [ 85.3],\n",
       "       [ 83.7],\n",
       "       [ 82.1],\n",
       "       [ 80.8],\n",
       "       [ 78.8],\n",
       "       [ 78.4],\n",
       "       [ 78. ],\n",
       "       [-99. ],\n",
       "       [ 80.6],\n",
       "       [ 79.2],\n",
       "       [ 77.9],\n",
       "       [ 80. ],\n",
       "       [ 84.2],\n",
       "       [ 84.2],\n",
       "       [ 82.9],\n",
       "       [ 81.5],\n",
       "       [ 80.3],\n",
       "       [ 80.4],\n",
       "       [ 82.1],\n",
       "       [ 83.3],\n",
       "       [ 86.1],\n",
       "       [ 83.6],\n",
       "       [ 82.8],\n",
       "       [ 80.9],\n",
       "       [ 80.9],\n",
       "       [ 81.6],\n",
       "       [ 83. ],\n",
       "       [ 84.5],\n",
       "       [ 86.9],\n",
       "       [ 85.9],\n",
       "       [ 85.6],\n",
       "       [ 83.3],\n",
       "       [ 85.2],\n",
       "       [ 84.6],\n",
       "       [ 87.1],\n",
       "       [ 89.5],\n",
       "       [ 90.1],\n",
       "       [ 89. ],\n",
       "       [ 88.6],\n",
       "       [ 87.6],\n",
       "       [ 86.2],\n",
       "       [ 85.9],\n",
       "       [ 87. ],\n",
       "       [ 88.7],\n",
       "       [ 87.4],\n",
       "       [ 87.5],\n",
       "       [ 88.3],\n",
       "       [ 86.7],\n",
       "       [ 86.4],\n",
       "       [ 86.3],\n",
       "       [ 85.9],\n",
       "       [ 86.4],\n",
       "       [ 86.9],\n",
       "       [ 86.3],\n",
       "       [ 86.4],\n",
       "       [ 86.9],\n",
       "       [ 87.1],\n",
       "       [ 86.8],\n",
       "       [ 86.5],\n",
       "       [ 86.3],\n",
       "       [ 86.2],\n",
       "       [ 85.6],\n",
       "       [ 85.7],\n",
       "       [ 85.9],\n",
       "       [ 85.5],\n",
       "       [ 86.6],\n",
       "       [ 87.5],\n",
       "       [ 87.6],\n",
       "       [ 88.7],\n",
       "       [ 88.6],\n",
       "       [ 87.6],\n",
       "       [ 87.2],\n",
       "       [ 87.6],\n",
       "       [ 87.9],\n",
       "       [ 87.6],\n",
       "       [ 87.2],\n",
       "       [ 88.3],\n",
       "       [ 87.6],\n",
       "       [ 87.9],\n",
       "       [ 88.1],\n",
       "       [ 88.8],\n",
       "       [ 88.5],\n",
       "       [ 88.9],\n",
       "       [ 89. ],\n",
       "       [ 88.4],\n",
       "       [ 86.7],\n",
       "       [ 87.2],\n",
       "       [ 85.8],\n",
       "       [ 87.9],\n",
       "       [ 86.5],\n",
       "       [ 85.9],\n",
       "       [ 86.4],\n",
       "       [ 87.1],\n",
       "       [ 87.7],\n",
       "       [ 86. ],\n",
       "       [ 85.7],\n",
       "       [ 86.7],\n",
       "       [ 87.6],\n",
       "       [ 84.8],\n",
       "       [ 86.2],\n",
       "       [ 86.9],\n",
       "       [ 87.3],\n",
       "       [ 86. ],\n",
       "       [ 86.5],\n",
       "       [ 87.3],\n",
       "       [ 86.8],\n",
       "       [ 86.5],\n",
       "       [ 82.3],\n",
       "       [ 83.7],\n",
       "       [ 82.9],\n",
       "       [ 80.6],\n",
       "       [ 80.6],\n",
       "       [ 81.5],\n",
       "       [ 82.2],\n",
       "       [ 82.9],\n",
       "       [ 80.6],\n",
       "       [ 81.1],\n",
       "       [ 77.7],\n",
       "       [ 82.6],\n",
       "       [ 81.4],\n",
       "       [ 81.7],\n",
       "       [ 80.9],\n",
       "       [ 83.1],\n",
       "       [ 82.4],\n",
       "       [ 83.1],\n",
       "       [ 82.9],\n",
       "       [ 81.8],\n",
       "       [ 83.5],\n",
       "       [ 83.4],\n",
       "       [ 80.5],\n",
       "       [ 81.5],\n",
       "       [ 82.7],\n",
       "       [ 81.3],\n",
       "       [ 82.4],\n",
       "       [ 82.3],\n",
       "       [ 83.3],\n",
       "       [ 83.9],\n",
       "       [ 84.4],\n",
       "       [ 83.4],\n",
       "       [ 83.3],\n",
       "       [ 83.6],\n",
       "       [ 83.2],\n",
       "       [ 83.5],\n",
       "       [ 84.2],\n",
       "       [ 83.7],\n",
       "       [ 83.8],\n",
       "       [ 82.6],\n",
       "       [ 83.9],\n",
       "       [ 83.9],\n",
       "       [ 84.4],\n",
       "       [ 84.4],\n",
       "       [ 83.9],\n",
       "       [ 83.6],\n",
       "       [ 85.2],\n",
       "       [ 85.5],\n",
       "       [ 85.7],\n",
       "       [ 83.8],\n",
       "       [ 84.5],\n",
       "       [ 82.6],\n",
       "       [ 80.7],\n",
       "       [ 79.2],\n",
       "       [ 81.2],\n",
       "       [ 80. ],\n",
       "       [ 78.4],\n",
       "       [ 79.2],\n",
       "       [ 82.9],\n",
       "       [ 82.7],\n",
       "       [ 81.8],\n",
       "       [ 78.5],\n",
       "       [ 81.1],\n",
       "       [ 81.8],\n",
       "       [ 81.6],\n",
       "       [ 81.5],\n",
       "       [ 82.7],\n",
       "       [ 81.1],\n",
       "       [ 78.6],\n",
       "       [ 81.1],\n",
       "       [ 82.5],\n",
       "       [ 82.1],\n",
       "       [ 83.6],\n",
       "       [ 84.5],\n",
       "       [ 82.9],\n",
       "       [ 83.5],\n",
       "       [ 83. ],\n",
       "       [ 80.3],\n",
       "       [ 78.6],\n",
       "       [ 81.5],\n",
       "       [ 82.6],\n",
       "       [ 81.3],\n",
       "       [ 82.1],\n",
       "       [ 83. ],\n",
       "       [ 83.7],\n",
       "       [ 82.3],\n",
       "       [ 81.9],\n",
       "       [ 81.8],\n",
       "       [ 81. ],\n",
       "       [ 81.6],\n",
       "       [ 82.8],\n",
       "       [ 83.8],\n",
       "       [ 85.5],\n",
       "       [ 87.2],\n",
       "       [ 84.1],\n",
       "       [ 84.4],\n",
       "       [ 83.5],\n",
       "       [ 85.8],\n",
       "       [ 84.4],\n",
       "       [ 83.4],\n",
       "       [ 83.8],\n",
       "       [ 82.8],\n",
       "       [ 83.2],\n",
       "       [ 80.3],\n",
       "       [ 81.5],\n",
       "       [ 82.3],\n",
       "       [ 83.9],\n",
       "       [ 84. ],\n",
       "       [ 85.9],\n",
       "       [ 82.6],\n",
       "       [ 84.2],\n",
       "       [ 84.7],\n",
       "       [ 84.5],\n",
       "       [ 83.3],\n",
       "       [ 82.9],\n",
       "       [ 83.4],\n",
       "       [ 80.7],\n",
       "       [ 82.5],\n",
       "       [ 83. ],\n",
       "       [ 78.3],\n",
       "       [ 79. ],\n",
       "       [ 78. ],\n",
       "       [ 79.4],\n",
       "       [ 79.7],\n",
       "       [ 78.9],\n",
       "       [ 77.5],\n",
       "       [ 78.9],\n",
       "       [ 79.4],\n",
       "       [ 81.2],\n",
       "       [ 81.2],\n",
       "       [ 80. ],\n",
       "       [ 77.3],\n",
       "       [ 76.2],\n",
       "       [ 77.9],\n",
       "       [ 77.5],\n",
       "       [ 77.6],\n",
       "       [ 78.6],\n",
       "       [ 77.7],\n",
       "       [ 78.3],\n",
       "       [ 81.1],\n",
       "       [ 80. ],\n",
       "       [ 79.5],\n",
       "       [ 77.2],\n",
       "       [ 78.1],\n",
       "       [ 76.2],\n",
       "       [ 75.7],\n",
       "       [ 76.3],\n",
       "       [ 76.5],\n",
       "       [ 76.4],\n",
       "       [ 74.1],\n",
       "       [ 73.8],\n",
       "       [ 74.9],\n",
       "       [ 77.2],\n",
       "       [ 78.5],\n",
       "       [ 80.2],\n",
       "       [ 76.3],\n",
       "       [ 76.1],\n",
       "       [ 77. ],\n",
       "       [ 77. ],\n",
       "       [ 76.7],\n",
       "       [ 75.7],\n",
       "       [ 77.8],\n",
       "       [ 76.6],\n",
       "       [ 76.5],\n",
       "       [ 75.9],\n",
       "       [ 78.3],\n",
       "       [ 74.8],\n",
       "       [ 76.8],\n",
       "       [ 77.7],\n",
       "       [ 75.9],\n",
       "       [ 71.4],\n",
       "       [ 71.3],\n",
       "       [ 77.2],\n",
       "       [ 77.4],\n",
       "       [ 78.1],\n",
       "       [ 75.6],\n",
       "       [ 76.2],\n",
       "       [ 75.9]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_scaled = sc.fit_transform(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90322581],\n",
       "       [0.90428345],\n",
       "       [0.8952935 ],\n",
       "       [0.89212057],\n",
       "       [0.9005817 ],\n",
       "       [0.91433104],\n",
       "       [0.90904283],\n",
       "       [0.89476467],\n",
       "       [0.8852459 ],\n",
       "       [0.88313062],\n",
       "       [0.89317821],\n",
       "       [0.89264939],\n",
       "       [0.89899524],\n",
       "       [0.91433104],\n",
       "       [0.91168694],\n",
       "       [0.89582232],\n",
       "       [0.91961925],\n",
       "       [0.91750397],\n",
       "       [0.90957166],\n",
       "       [0.90375463],\n",
       "       [0.9005817 ],\n",
       "       [0.91115812],\n",
       "       [0.90111052],\n",
       "       [0.9005817 ],\n",
       "       [0.89476467],\n",
       "       [0.90957166],\n",
       "       [0.91380222],\n",
       "       [0.93336859],\n",
       "       [0.92966684],\n",
       "       [0.9259651 ],\n",
       "       [0.93759915],\n",
       "       [0.94077208],\n",
       "       [0.93759915],\n",
       "       [0.91115812],\n",
       "       [0.91856161],\n",
       "       [0.93231095],\n",
       "       [0.94764675],\n",
       "       [0.96351137],\n",
       "       [0.95187731],\n",
       "       [0.94024326],\n",
       "       [0.93495505],\n",
       "       [0.92966684],\n",
       "       [0.92490746],\n",
       "       [0.92384981],\n",
       "       [0.91538868],\n",
       "       [0.91168694],\n",
       "       [0.91750397],\n",
       "       [0.92173453],\n",
       "       [0.90586991],\n",
       "       [0.91010048],\n",
       "       [0.93019566],\n",
       "       [0.93548387],\n",
       "       [0.91803279],\n",
       "       [0.92649392],\n",
       "       [0.90586991],\n",
       "       [0.9259651 ],\n",
       "       [0.92649392],\n",
       "       [0.92173453],\n",
       "       [0.91433104],\n",
       "       [0.9159175 ],\n",
       "       [0.93125331],\n",
       "       [0.94447382],\n",
       "       [0.94024326],\n",
       "       [0.93971444],\n",
       "       [0.93336859],\n",
       "       [0.93072448],\n",
       "       [0.92808038],\n",
       "       [0.92543628],\n",
       "       [0.93601269],\n",
       "       [0.93654151],\n",
       "       [0.95293496],\n",
       "       [0.94024326],\n",
       "       [0.94553146],\n",
       "       [0.94606029],\n",
       "       [0.95029085],\n",
       "       [0.93971444],\n",
       "       [0.93389741],\n",
       "       [0.94235854],\n",
       "       [0.95187731],\n",
       "       [0.98096245],\n",
       "       [0.9746166 ],\n",
       "       [0.96615547],\n",
       "       [0.95769434],\n",
       "       [0.95081967],\n",
       "       [0.94024326],\n",
       "       [0.93812797],\n",
       "       [0.93601269],\n",
       "       [0.        ],\n",
       "       [0.94976203],\n",
       "       [0.94235854],\n",
       "       [0.93548387],\n",
       "       [0.94658911],\n",
       "       [0.96879958],\n",
       "       [0.96879958],\n",
       "       [0.96192491],\n",
       "       [0.95452142],\n",
       "       [0.94817557],\n",
       "       [0.94870439],\n",
       "       [0.95769434],\n",
       "       [0.96404019],\n",
       "       [0.97884717],\n",
       "       [0.96562665],\n",
       "       [0.96139609],\n",
       "       [0.95134849],\n",
       "       [0.95134849],\n",
       "       [0.95505024],\n",
       "       [0.96245373],\n",
       "       [0.97038604],\n",
       "       [0.98307774],\n",
       "       [0.97778953],\n",
       "       [0.97620307],\n",
       "       [0.96404019],\n",
       "       [0.97408778],\n",
       "       [0.97091486],\n",
       "       [0.98413538],\n",
       "       [0.99682708],\n",
       "       [1.        ],\n",
       "       [0.99418297],\n",
       "       [0.99206769],\n",
       "       [0.98677948],\n",
       "       [0.97937599],\n",
       "       [0.97778953],\n",
       "       [0.98360656],\n",
       "       [0.99259651],\n",
       "       [0.98572184],\n",
       "       [0.98625066],\n",
       "       [0.99048123],\n",
       "       [0.9820201 ],\n",
       "       [0.98043363],\n",
       "       [0.97990481],\n",
       "       [0.97778953],\n",
       "       [0.98043363],\n",
       "       [0.98307774],\n",
       "       [0.97990481],\n",
       "       [0.98043363],\n",
       "       [0.98307774],\n",
       "       [0.98413538],\n",
       "       [0.98254892],\n",
       "       [0.98096245],\n",
       "       [0.97990481],\n",
       "       [0.97937599],\n",
       "       [0.97620307],\n",
       "       [0.97673189],\n",
       "       [0.97778953],\n",
       "       [0.97567425],\n",
       "       [0.98149127],\n",
       "       [0.98625066],\n",
       "       [0.98677948],\n",
       "       [0.99259651],\n",
       "       [0.99206769],\n",
       "       [0.98677948],\n",
       "       [0.9846642 ],\n",
       "       [0.98677948],\n",
       "       [0.98836594],\n",
       "       [0.98677948],\n",
       "       [0.9846642 ],\n",
       "       [0.99048123],\n",
       "       [0.98677948],\n",
       "       [0.98836594],\n",
       "       [0.98942359],\n",
       "       [0.99312533],\n",
       "       [0.99153887],\n",
       "       [0.99365415],\n",
       "       [0.99418297],\n",
       "       [0.99101005],\n",
       "       [0.9820201 ],\n",
       "       [0.9846642 ],\n",
       "       [0.97726071],\n",
       "       [0.98836594],\n",
       "       [0.98096245],\n",
       "       [0.97778953],\n",
       "       [0.98043363],\n",
       "       [0.98413538],\n",
       "       [0.9873083 ],\n",
       "       [0.97831835],\n",
       "       [0.97673189],\n",
       "       [0.9820201 ],\n",
       "       [0.98677948],\n",
       "       [0.9719725 ],\n",
       "       [0.97937599],\n",
       "       [0.98307774],\n",
       "       [0.98519302],\n",
       "       [0.97831835],\n",
       "       [0.98096245],\n",
       "       [0.98519302],\n",
       "       [0.98254892],\n",
       "       [0.98096245],\n",
       "       [0.95875198],\n",
       "       [0.96615547],\n",
       "       [0.96192491],\n",
       "       [0.94976203],\n",
       "       [0.94976203],\n",
       "       [0.95452142],\n",
       "       [0.95822316],\n",
       "       [0.96192491],\n",
       "       [0.94976203],\n",
       "       [0.95240613],\n",
       "       [0.93442623],\n",
       "       [0.96033845],\n",
       "       [0.9539926 ],\n",
       "       [0.95557906],\n",
       "       [0.95134849],\n",
       "       [0.96298255],\n",
       "       [0.9592808 ],\n",
       "       [0.96298255],\n",
       "       [0.96192491],\n",
       "       [0.95610788],\n",
       "       [0.96509783],\n",
       "       [0.96456901],\n",
       "       [0.94923321],\n",
       "       [0.95452142],\n",
       "       [0.96086727],\n",
       "       [0.95346378],\n",
       "       [0.9592808 ],\n",
       "       [0.95875198],\n",
       "       [0.96404019],\n",
       "       [0.96721311],\n",
       "       [0.96985722],\n",
       "       [0.96456901],\n",
       "       [0.96404019],\n",
       "       [0.96562665],\n",
       "       [0.96351137],\n",
       "       [0.96509783],\n",
       "       [0.96879958],\n",
       "       [0.96615547],\n",
       "       [0.96668429],\n",
       "       [0.96033845],\n",
       "       [0.96721311],\n",
       "       [0.96721311],\n",
       "       [0.96985722],\n",
       "       [0.96985722],\n",
       "       [0.96721311],\n",
       "       [0.96562665],\n",
       "       [0.97408778],\n",
       "       [0.97567425],\n",
       "       [0.97673189],\n",
       "       [0.96668429],\n",
       "       [0.97038604],\n",
       "       [0.96033845],\n",
       "       [0.95029085],\n",
       "       [0.94235854],\n",
       "       [0.95293496],\n",
       "       [0.94658911],\n",
       "       [0.93812797],\n",
       "       [0.94235854],\n",
       "       [0.96192491],\n",
       "       [0.96086727],\n",
       "       [0.95610788],\n",
       "       [0.9386568 ],\n",
       "       [0.95240613],\n",
       "       [0.95610788],\n",
       "       [0.95505024],\n",
       "       [0.95452142],\n",
       "       [0.96086727],\n",
       "       [0.95240613],\n",
       "       [0.93918562],\n",
       "       [0.95240613],\n",
       "       [0.95980962],\n",
       "       [0.95769434],\n",
       "       [0.96562665],\n",
       "       [0.97038604],\n",
       "       [0.96192491],\n",
       "       [0.96509783],\n",
       "       [0.96245373],\n",
       "       [0.94817557],\n",
       "       [0.93918562],\n",
       "       [0.95452142],\n",
       "       [0.96033845],\n",
       "       [0.95346378],\n",
       "       [0.95769434],\n",
       "       [0.96245373],\n",
       "       [0.96615547],\n",
       "       [0.95875198],\n",
       "       [0.9566367 ],\n",
       "       [0.95610788],\n",
       "       [0.95187731],\n",
       "       [0.95505024],\n",
       "       [0.96139609],\n",
       "       [0.96668429],\n",
       "       [0.97567425],\n",
       "       [0.9846642 ],\n",
       "       [0.96827076],\n",
       "       [0.96985722],\n",
       "       [0.96509783],\n",
       "       [0.97726071],\n",
       "       [0.96985722],\n",
       "       [0.96456901],\n",
       "       [0.96668429],\n",
       "       [0.96139609],\n",
       "       [0.96351137],\n",
       "       [0.94817557],\n",
       "       [0.95452142],\n",
       "       [0.95875198],\n",
       "       [0.96721311],\n",
       "       [0.96774194],\n",
       "       [0.97778953],\n",
       "       [0.96033845],\n",
       "       [0.96879958],\n",
       "       [0.97144368],\n",
       "       [0.97038604],\n",
       "       [0.96404019],\n",
       "       [0.96192491],\n",
       "       [0.96456901],\n",
       "       [0.95029085],\n",
       "       [0.95980962],\n",
       "       [0.96245373],\n",
       "       [0.93759915],\n",
       "       [0.9413009 ],\n",
       "       [0.93601269],\n",
       "       [0.94341618],\n",
       "       [0.94500264],\n",
       "       [0.94077208],\n",
       "       [0.93336859],\n",
       "       [0.94077208],\n",
       "       [0.94341618],\n",
       "       [0.95293496],\n",
       "       [0.95293496],\n",
       "       [0.94658911],\n",
       "       [0.93231095],\n",
       "       [0.92649392],\n",
       "       [0.93548387],\n",
       "       [0.93336859],\n",
       "       [0.93389741],\n",
       "       [0.93918562],\n",
       "       [0.93442623],\n",
       "       [0.93759915],\n",
       "       [0.95240613],\n",
       "       [0.94658911],\n",
       "       [0.943945  ],\n",
       "       [0.93178213],\n",
       "       [0.93654151],\n",
       "       [0.92649392],\n",
       "       [0.92384981],\n",
       "       [0.92702274],\n",
       "       [0.92808038],\n",
       "       [0.92755156],\n",
       "       [0.91538868],\n",
       "       [0.91380222],\n",
       "       [0.91961925],\n",
       "       [0.93178213],\n",
       "       [0.9386568 ],\n",
       "       [0.94764675],\n",
       "       [0.92702274],\n",
       "       [0.9259651 ],\n",
       "       [0.93072448],\n",
       "       [0.93072448],\n",
       "       [0.92913802],\n",
       "       [0.92384981],\n",
       "       [0.93495505],\n",
       "       [0.9286092 ],\n",
       "       [0.92808038],\n",
       "       [0.92490746],\n",
       "       [0.93759915],\n",
       "       [0.91909043],\n",
       "       [0.92966684],\n",
       "       [0.93442623],\n",
       "       [0.92490746],\n",
       "       [0.90111052],\n",
       "       [0.9005817 ],\n",
       "       [0.93178213],\n",
       "       [0.93283977],\n",
       "       [0.93654151],\n",
       "       [0.92332099],\n",
       "       [0.92649392],\n",
       "       [0.92490746]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "n_future = 4 # next 4 days temperature forecast\n",
    "n_past = 30 # Past 30 days \n",
    "for i in range(0,len(training_set_scaled)-n_past-n_future+1):\n",
    "    x_train.append(training_set_scaled[i : i + n_past , 0])     \n",
    "    y_train.append(training_set_scaled[i + n_past : i + n_past + n_future , 0 ])\n",
    "x_train , y_train = np.array(x_train), np.array(y_train)\n",
    "x_train = np.reshape(x_train, (x_train.shape[0] , x_train.shape[1], 1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM,Dense ,Dropout\n",
    "# Fitting RNN to training set using Keras Callbacks. Read Keras callbacks docs for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\KABILAN\\anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 265 samples, validate on 67 samples\n",
      "Epoch 1/500\n",
      "265/265 [==============================] - 5s 17ms/step - loss: 0.6031 - acc: 0.3057 - val_loss: 0.0634 - val_acc: 0.3134\n",
      "Epoch 2/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0866 - acc: 0.3170 - val_loss: 0.0103 - val_acc: 0.2239\n",
      "Epoch 3/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0465 - acc: 0.2415 - val_loss: 0.0032 - val_acc: 0.2239\n",
      "Epoch 4/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0396 - acc: 0.2075 - val_loss: 0.0045 - val_acc: 0.2388\n",
      "Epoch 5/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0329 - acc: 0.2830 - val_loss: 9.8997e-04 - val_acc: 0.2239\n",
      "Epoch 6/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0327 - acc: 0.2528 - val_loss: 6.9600e-04 - val_acc: 0.3134\n",
      "Epoch 7/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0291 - acc: 0.2679 - val_loss: 0.0013 - val_acc: 0.2388\n",
      "Epoch 8/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0254 - acc: 0.2566 - val_loss: 3.0578e-04 - val_acc: 0.2239\n",
      "Epoch 9/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0258 - acc: 0.2491 - val_loss: 0.0014 - val_acc: 0.2239\n",
      "Epoch 10/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0282 - acc: 0.2679 - val_loss: 3.0446e-04 - val_acc: 0.2388\n",
      "Epoch 11/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0241 - acc: 0.2717 - val_loss: 3.5453e-04 - val_acc: 0.2388\n",
      "Epoch 12/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0245 - acc: 0.2868 - val_loss: 9.2141e-04 - val_acc: 0.2239\n",
      "Epoch 13/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0253 - acc: 0.2792 - val_loss: 2.5089e-04 - val_acc: 0.3134\n",
      "Epoch 14/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0244 - acc: 0.2755 - val_loss: 4.6330e-04 - val_acc: 0.2388\n",
      "Epoch 15/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0258 - acc: 0.2566 - val_loss: 0.0018 - val_acc: 0.2537\n",
      "Epoch 16/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0229 - acc: 0.1849 - val_loss: 2.6865e-04 - val_acc: 0.2985\n",
      "Epoch 17/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0229 - acc: 0.2453 - val_loss: 0.0022 - val_acc: 0.3134\n",
      "Epoch 18/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.2453 - val_loss: 3.3938e-04 - val_acc: 0.3134\n",
      "Epoch 19/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0202 - acc: 0.2189 - val_loss: 2.8406e-04 - val_acc: 0.2239\n",
      "Epoch 20/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.2453 - val_loss: 0.0013 - val_acc: 0.3134\n",
      "Epoch 21/500\n",
      "265/265 [==============================] - ETA: 0s - loss: 0.0212 - acc: 0.281 - 1s 3ms/step - loss: 0.0210 - acc: 0.2830 - val_loss: 6.5987e-04 - val_acc: 0.3134\n",
      "Epoch 22/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0196 - acc: 0.2302 - val_loss: 0.0011 - val_acc: 0.2836\n",
      "Epoch 23/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0194 - acc: 0.2340 - val_loss: 7.2631e-04 - val_acc: 0.2388\n",
      "Epoch 24/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0199 - acc: 0.2453 - val_loss: 7.2572e-04 - val_acc: 0.2239\n",
      "Epoch 25/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0212 - acc: 0.2566 - val_loss: 9.0858e-04 - val_acc: 0.2239\n",
      "Epoch 26/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0238 - acc: 0.3094 - val_loss: 9.5445e-04 - val_acc: 0.2239\n",
      "Epoch 27/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0190 - acc: 0.2566 - val_loss: 6.3171e-04 - val_acc: 0.3134\n",
      "Epoch 28/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0193 - acc: 0.2189 - val_loss: 4.8229e-04 - val_acc: 0.2239\n",
      "Epoch 29/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0193 - acc: 0.2453 - val_loss: 8.6608e-04 - val_acc: 0.2239\n",
      "Epoch 30/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0205 - acc: 0.2604 - val_loss: 6.1233e-04 - val_acc: 0.2239\n",
      "Epoch 31/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0177 - acc: 0.2679 - val_loss: 2.8243e-04 - val_acc: 0.2239\n",
      "Epoch 32/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0168 - acc: 0.2189 - val_loss: 9.2104e-04 - val_acc: 0.2388\n",
      "Epoch 33/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0185 - acc: 0.2264 - val_loss: 3.2941e-04 - val_acc: 0.2388\n",
      "Epoch 34/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0190 - acc: 0.2604 - val_loss: 0.0021 - val_acc: 0.2239\n",
      "Epoch 35/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0183 - acc: 0.2830 - val_loss: 4.6413e-04 - val_acc: 0.2239\n",
      "Epoch 36/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0171 - acc: 0.2264 - val_loss: 4.0571e-04 - val_acc: 0.2388\n",
      "Epoch 37/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0184 - acc: 0.2679 - val_loss: 0.0021 - val_acc: 0.2239\n",
      "Epoch 38/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0176 - acc: 0.3208 - val_loss: 3.5314e-04 - val_acc: 0.3134\n",
      "Epoch 39/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0186 - acc: 0.2906 - val_loss: 6.2042e-04 - val_acc: 0.3134\n",
      "Epoch 40/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0175 - acc: 0.2566 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 41/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0159 - acc: 0.2755 - val_loss: 5.6769e-04 - val_acc: 0.3134\n",
      "Epoch 42/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0176 - acc: 0.2566 - val_loss: 4.4291e-04 - val_acc: 0.3134\n",
      "Epoch 43/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0172 - acc: 0.2792 - val_loss: 3.2871e-04 - val_acc: 0.2239\n",
      "Epoch 44/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0166 - acc: 0.2792 - val_loss: 5.4872e-04 - val_acc: 0.2388\n",
      "Epoch 45/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0170 - acc: 0.2038 - val_loss: 8.9038e-04 - val_acc: 0.3134\n",
      "Epoch 46/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0188 - acc: 0.2566 - val_loss: 0.0011 - val_acc: 0.3134\n",
      "Epoch 47/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0165 - acc: 0.2717 - val_loss: 2.8462e-04 - val_acc: 0.2388\n",
      "Epoch 48/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0168 - acc: 0.2377 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 49/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0169 - acc: 0.2981 - val_loss: 4.4318e-04 - val_acc: 0.2388\n",
      "Epoch 50/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0162 - acc: 0.2566 - val_loss: 5.6443e-04 - val_acc: 0.2239\n",
      "Epoch 51/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0171 - acc: 0.2566 - val_loss: 8.6953e-04 - val_acc: 0.2239\n",
      "Epoch 52/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0164 - acc: 0.2943 - val_loss: 2.1659e-04 - val_acc: 0.2239\n",
      "Epoch 53/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0150 - acc: 0.2340 - val_loss: 0.0025 - val_acc: 0.2388\n",
      "Epoch 54/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0169 - acc: 0.2868 - val_loss: 3.9301e-04 - val_acc: 0.2239\n",
      "Epoch 55/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0151 - acc: 0.2604 - val_loss: 0.0016 - val_acc: 0.2239\n",
      "Epoch 56/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0156 - acc: 0.2340 - val_loss: 3.9534e-04 - val_acc: 0.2239\n",
      "Epoch 57/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0153 - acc: 0.2491 - val_loss: 9.6056e-04 - val_acc: 0.2239\n",
      "Epoch 58/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0152 - acc: 0.2642 - val_loss: 3.3799e-04 - val_acc: 0.2239\n",
      "Epoch 59/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0155 - acc: 0.2491 - val_loss: 4.8223e-04 - val_acc: 0.2239\n",
      "Epoch 60/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0167 - acc: 0.2566 - val_loss: 0.0015 - val_acc: 0.2388\n",
      "Epoch 61/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0149 - acc: 0.2755 - val_loss: 8.6672e-04 - val_acc: 0.2239\n",
      "Epoch 62/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0172 - acc: 0.2679 - val_loss: 3.0470e-04 - val_acc: 0.2239\n",
      "Epoch 63/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0163 - acc: 0.2566 - val_loss: 0.0026 - val_acc: 0.2388\n",
      "Epoch 64/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0162 - acc: 0.2528 - val_loss: 3.1095e-04 - val_acc: 0.3134\n",
      "Epoch 65/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0141 - acc: 0.2075 - val_loss: 0.0020 - val_acc: 0.3134\n",
      "Epoch 66/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0150 - acc: 0.1925 - val_loss: 2.4829e-04 - val_acc: 0.2239\n",
      "Epoch 67/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0147 - acc: 0.2302 - val_loss: 0.0013 - val_acc: 0.2239\n",
      "Epoch 68/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0152 - acc: 0.2377 - val_loss: 3.1094e-04 - val_acc: 0.3134\n",
      "Epoch 69/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0132 - acc: 0.2642 - val_loss: 8.5186e-04 - val_acc: 0.2239\n",
      "Epoch 70/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0151 - acc: 0.2717 - val_loss: 0.0010 - val_acc: 0.2239\n",
      "Epoch 71/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0131 - acc: 0.2377 - val_loss: 4.2728e-04 - val_acc: 0.2239\n",
      "Epoch 72/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0152 - acc: 0.2717 - val_loss: 0.0013 - val_acc: 0.2239\n",
      "Epoch 73/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0124 - acc: 0.2679 - val_loss: 2.6956e-04 - val_acc: 0.3134\n",
      "Epoch 74/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0141 - acc: 0.2377 - val_loss: 4.1988e-04 - val_acc: 0.2239\n",
      "Epoch 75/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0144 - acc: 0.2264 - val_loss: 5.6784e-04 - val_acc: 0.3134\n",
      "Epoch 76/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0120 - acc: 0.2717 - val_loss: 4.5721e-04 - val_acc: 0.2388\n",
      "Epoch 77/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0138 - acc: 0.2453 - val_loss: 4.1578e-04 - val_acc: 0.2239\n",
      "Epoch 78/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0149 - acc: 0.2000 - val_loss: 5.0103e-04 - val_acc: 0.2239\n",
      "Epoch 79/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0143 - acc: 0.2491 - val_loss: 2.1777e-04 - val_acc: 0.2239\n",
      "Epoch 80/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0135 - acc: 0.2302 - val_loss: 0.0015 - val_acc: 0.2239\n",
      "Epoch 81/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0134 - acc: 0.2151 - val_loss: 2.4462e-04 - val_acc: 0.3134\n",
      "Epoch 82/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0131 - acc: 0.2340 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 83/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0144 - acc: 0.2075 - val_loss: 2.4999e-04 - val_acc: 0.2239\n",
      "Epoch 84/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0122 - acc: 0.2151 - val_loss: 6.7456e-04 - val_acc: 0.2388\n",
      "Epoch 85/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0137 - acc: 0.2415 - val_loss: 9.2773e-04 - val_acc: 0.2239\n",
      "Epoch 86/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0131 - acc: 0.2151 - val_loss: 9.5193e-04 - val_acc: 0.2239\n",
      "Epoch 87/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0134 - acc: 0.2453 - val_loss: 6.5371e-04 - val_acc: 0.2388\n",
      "Epoch 88/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0142 - acc: 0.2830 - val_loss: 9.5204e-04 - val_acc: 0.2388\n",
      "Epoch 89/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0135 - acc: 0.2755 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 90/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0123 - acc: 0.2075 - val_loss: 6.6918e-04 - val_acc: 0.3134\n",
      "Epoch 91/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0135 - acc: 0.2906 - val_loss: 3.7134e-04 - val_acc: 0.2239\n",
      "Epoch 92/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0136 - acc: 0.2491 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 93/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0131 - acc: 0.2566 - val_loss: 2.1975e-04 - val_acc: 0.2239\n",
      "Epoch 94/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0141 - acc: 0.2302 - val_loss: 0.0013 - val_acc: 0.3134\n",
      "Epoch 95/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0128 - acc: 0.2302 - val_loss: 6.4242e-04 - val_acc: 0.2239\n",
      "Epoch 96/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0132 - acc: 0.1849 - val_loss: 9.2867e-04 - val_acc: 0.2239\n",
      "Epoch 97/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0148 - acc: 0.2189 - val_loss: 2.9835e-04 - val_acc: 0.2239\n",
      "Epoch 98/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0125 - acc: 0.2302 - val_loss: 9.8820e-04 - val_acc: 0.2239\n",
      "Epoch 99/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0116 - acc: 0.2113 - val_loss: 3.2170e-04 - val_acc: 0.2388\n",
      "Epoch 100/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0120 - acc: 0.2226 - val_loss: 6.2979e-04 - val_acc: 0.2239\n",
      "Epoch 101/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0116 - acc: 0.2340 - val_loss: 8.9917e-04 - val_acc: 0.2388\n",
      "Epoch 102/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0138 - acc: 0.3019 - val_loss: 2.3191e-04 - val_acc: 0.2239\n",
      "Epoch 103/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0118 - acc: 0.2679 - val_loss: 0.0018 - val_acc: 0.3134\n",
      "Epoch 104/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0119 - acc: 0.2642 - val_loss: 2.1776e-04 - val_acc: 0.2388\n",
      "Epoch 105/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0132 - acc: 0.2528 - val_loss: 0.0028 - val_acc: 0.2388\n",
      "Epoch 106/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0141 - acc: 0.2755 - val_loss: 2.2035e-04 - val_acc: 0.3134\n",
      "Epoch 107/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0124 - acc: 0.2340 - val_loss: 8.0725e-04 - val_acc: 0.2239\n",
      "Epoch 108/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0118 - acc: 0.2642 - val_loss: 4.7744e-04 - val_acc: 0.2239\n",
      "Epoch 109/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0124 - acc: 0.2528 - val_loss: 4.0303e-04 - val_acc: 0.2239\n",
      "Epoch 110/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0119 - acc: 0.2377 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 111/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0134 - acc: 0.2528 - val_loss: 7.9583e-04 - val_acc: 0.2388\n",
      "Epoch 112/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0111 - acc: 0.2642 - val_loss: 6.4643e-04 - val_acc: 0.3134\n",
      "Epoch 113/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0116 - acc: 0.2679 - val_loss: 0.0014 - val_acc: 0.3134\n",
      "Epoch 114/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0141 - acc: 0.2604 - val_loss: 4.4349e-04 - val_acc: 0.2388\n",
      "Epoch 115/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0114 - acc: 0.2000 - val_loss: 9.7653e-04 - val_acc: 0.2239\n",
      "Epoch 116/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0118 - acc: 0.2642 - val_loss: 2.2126e-04 - val_acc: 0.2239\n",
      "Epoch 117/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0113 - acc: 0.2491 - val_loss: 0.0011 - val_acc: 0.2388\n",
      "Epoch 118/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0109 - acc: 0.2642 - val_loss: 5.0082e-04 - val_acc: 0.2388\n",
      "Epoch 119/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0118 - acc: 0.2679 - val_loss: 8.0060e-04 - val_acc: 0.2388\n",
      "Epoch 120/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0123 - acc: 0.2075 - val_loss: 2.2230e-04 - val_acc: 0.2239\n",
      "Epoch 121/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0120 - acc: 0.2226 - val_loss: 5.8127e-04 - val_acc: 0.2239\n",
      "Epoch 122/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0117 - acc: 0.2340 - val_loss: 0.0013 - val_acc: 0.2239\n",
      "Epoch 123/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0119 - acc: 0.3019 - val_loss: 2.8748e-04 - val_acc: 0.2388\n",
      "Epoch 124/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0113 - acc: 0.2113 - val_loss: 4.5144e-04 - val_acc: 0.2388\n",
      "Epoch 125/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0113 - acc: 0.2038 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 126/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0116 - acc: 0.2755 - val_loss: 3.5866e-04 - val_acc: 0.2239\n",
      "Epoch 127/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0109 - acc: 0.2679 - val_loss: 9.8945e-04 - val_acc: 0.3134\n",
      "Epoch 128/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0123 - acc: 0.2679 - val_loss: 2.5435e-04 - val_acc: 0.2687\n",
      "Epoch 129/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0107 - acc: 0.1774 - val_loss: 0.0014 - val_acc: 0.2239\n",
      "Epoch 130/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0105 - acc: 0.2566 - val_loss: 2.2173e-04 - val_acc: 0.2239\n",
      "Epoch 131/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0105 - acc: 0.2604 - val_loss: 0.0017 - val_acc: 0.2239\n",
      "Epoch 132/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0109 - acc: 0.2377 - val_loss: 2.2677e-04 - val_acc: 0.2239\n",
      "Epoch 133/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0101 - acc: 0.2528 - val_loss: 7.3532e-04 - val_acc: 0.2239\n",
      "Epoch 134/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0107 - acc: 0.2491 - val_loss: 5.5434e-04 - val_acc: 0.2239\n",
      "Epoch 135/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0109 - acc: 0.2453 - val_loss: 8.4434e-04 - val_acc: 0.2388\n",
      "Epoch 136/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0111 - acc: 0.2113 - val_loss: 2.2742e-04 - val_acc: 0.2239\n",
      "Epoch 137/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0116 - acc: 0.2189 - val_loss: 0.0013 - val_acc: 0.2239\n",
      "Epoch 138/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0104 - acc: 0.2528 - val_loss: 3.9138e-04 - val_acc: 0.3134\n",
      "Epoch 139/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0111 - acc: 0.2679 - val_loss: 0.0014 - val_acc: 0.2388\n",
      "Epoch 140/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0112 - acc: 0.2491 - val_loss: 4.7592e-04 - val_acc: 0.2239\n",
      "Epoch 141/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0106 - acc: 0.2528 - val_loss: 8.5739e-04 - val_acc: 0.2239\n",
      "Epoch 142/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0100 - acc: 0.2453 - val_loss: 2.5010e-04 - val_acc: 0.2239\n",
      "Epoch 143/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0115 - acc: 0.2377 - val_loss: 0.0015 - val_acc: 0.2388\n",
      "Epoch 144/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0106 - acc: 0.2189 - val_loss: 2.1453e-04 - val_acc: 0.2388\n",
      "Epoch 145/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0108 - acc: 0.2377 - val_loss: 0.0019 - val_acc: 0.2239\n",
      "Epoch 146/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0106 - acc: 0.2226 - val_loss: 2.3914e-04 - val_acc: 0.2239\n",
      "Epoch 147/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0115 - acc: 0.2642 - val_loss: 9.6453e-04 - val_acc: 0.2239\n",
      "Epoch 148/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0107 - acc: 0.2528 - val_loss: 3.9561e-04 - val_acc: 0.2388\n",
      "Epoch 149/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0102 - acc: 0.2566 - val_loss: 5.8626e-04 - val_acc: 0.2239\n",
      "Epoch 150/500\n",
      "265/265 [==============================] - 1s 4ms/step - loss: 0.0102 - acc: 0.2000 - val_loss: 6.4220e-04 - val_acc: 0.2239\n",
      "Epoch 151/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0114 - acc: 0.2566 - val_loss: 6.1059e-04 - val_acc: 0.2239\n",
      "Epoch 152/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0102 - acc: 0.2830 - val_loss: 2.8466e-04 - val_acc: 0.3134\n",
      "Epoch 153/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0101 - acc: 0.2906 - val_loss: 5.3803e-04 - val_acc: 0.2388\n",
      "Epoch 154/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0108 - acc: 0.2415 - val_loss: 4.4482e-04 - val_acc: 0.3134\n",
      "Epoch 155/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0092 - acc: 0.3283 - val_loss: 3.1340e-04 - val_acc: 0.3134\n",
      "Epoch 156/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0115 - acc: 0.2491 - val_loss: 0.0018 - val_acc: 0.2239\n",
      "Epoch 157/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0105 - acc: 0.2717 - val_loss: 2.1805e-04 - val_acc: 0.2239\n",
      "Epoch 158/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0103 - acc: 0.2189 - val_loss: 9.0803e-04 - val_acc: 0.2239\n",
      "Epoch 159/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0108 - acc: 0.2377 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 160/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0097 - acc: 0.2906 - val_loss: 5.9945e-04 - val_acc: 0.2239\n",
      "Epoch 161/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0101 - acc: 0.2679 - val_loss: 3.6406e-04 - val_acc: 0.2239\n",
      "Epoch 162/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0099 - acc: 0.2642 - val_loss: 6.8859e-04 - val_acc: 0.2239\n",
      "Epoch 163/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0101 - acc: 0.2302 - val_loss: 6.7760e-04 - val_acc: 0.2239\n",
      "Epoch 164/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0099 - acc: 0.2113 - val_loss: 3.6882e-04 - val_acc: 0.2239\n",
      "Epoch 165/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0094 - acc: 0.2528 - val_loss: 9.0047e-04 - val_acc: 0.2239\n",
      "Epoch 166/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0093 - acc: 0.2491 - val_loss: 3.4671e-04 - val_acc: 0.2388\n",
      "Epoch 167/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0101 - acc: 0.3019 - val_loss: 8.1516e-04 - val_acc: 0.2239\n",
      "Epoch 168/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0095 - acc: 0.2566 - val_loss: 6.7127e-04 - val_acc: 0.2239\n",
      "Epoch 169/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0093 - acc: 0.2038 - val_loss: 6.2301e-04 - val_acc: 0.2239\n",
      "Epoch 170/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0112 - acc: 0.2642 - val_loss: 0.0011 - val_acc: 0.3134\n",
      "Epoch 171/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0098 - acc: 0.2302 - val_loss: 2.7475e-04 - val_acc: 0.2388\n",
      "Epoch 172/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0090 - acc: 0.2340 - val_loss: 4.4617e-04 - val_acc: 0.2388\n",
      "Epoch 173/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0094 - acc: 0.2075 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 174/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0106 - acc: 0.2038 - val_loss: 2.5254e-04 - val_acc: 0.3134\n",
      "Epoch 175/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0101 - acc: 0.2491 - val_loss: 0.0015 - val_acc: 0.2239\n",
      "Epoch 176/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0098 - acc: 0.2566 - val_loss: 3.2636e-04 - val_acc: 0.2239\n",
      "Epoch 177/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0101 - acc: 0.2679 - val_loss: 5.8405e-04 - val_acc: 0.2239\n",
      "Epoch 178/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0089 - acc: 0.2189 - val_loss: 2.2909e-04 - val_acc: 0.2239\n",
      "Epoch 179/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0100 - acc: 0.2566 - val_loss: 0.0012 - val_acc: 0.3134\n",
      "Epoch 180/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0111 - acc: 0.2264 - val_loss: 2.5998e-04 - val_acc: 0.2239\n",
      "Epoch 181/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0097 - acc: 0.2642 - val_loss: 7.6628e-04 - val_acc: 0.2388\n",
      "Epoch 182/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0100 - acc: 0.2415 - val_loss: 3.4077e-04 - val_acc: 0.2388\n",
      "Epoch 183/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0090 - acc: 0.2453 - val_loss: 9.2151e-04 - val_acc: 0.2239\n",
      "Epoch 184/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0105 - acc: 0.2415 - val_loss: 3.9290e-04 - val_acc: 0.2239\n",
      "Epoch 185/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0091 - acc: 0.2792 - val_loss: 7.5664e-04 - val_acc: 0.2239\n",
      "Epoch 186/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0089 - acc: 0.2528 - val_loss: 3.8494e-04 - val_acc: 0.2239\n",
      "Epoch 187/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0086 - acc: 0.2000 - val_loss: 4.1677e-04 - val_acc: 0.2239\n",
      "Epoch 188/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0091 - acc: 0.2642 - val_loss: 8.3864e-04 - val_acc: 0.3134\n",
      "Epoch 189/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0093 - acc: 0.3019 - val_loss: 5.4317e-04 - val_acc: 0.2239\n",
      "Epoch 190/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0090 - acc: 0.2830 - val_loss: 6.4698e-04 - val_acc: 0.2239\n",
      "Epoch 191/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0089 - acc: 0.2151 - val_loss: 6.3682e-04 - val_acc: 0.2239\n",
      "Epoch 192/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0095 - acc: 0.2566 - val_loss: 8.7314e-04 - val_acc: 0.2239\n",
      "Epoch 193/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0095 - acc: 0.2151 - val_loss: 5.9694e-04 - val_acc: 0.2239\n",
      "Epoch 194/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0080 - acc: 0.2717 - val_loss: 7.0018e-04 - val_acc: 0.2239\n",
      "Epoch 195/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0082 - acc: 0.2189 - val_loss: 2.8417e-04 - val_acc: 0.2239\n",
      "Epoch 196/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0088 - acc: 0.2679 - val_loss: 0.0016 - val_acc: 0.2239\n",
      "Epoch 197/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0085 - acc: 0.2075 - val_loss: 3.1362e-04 - val_acc: 0.2388\n",
      "Epoch 198/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0091 - acc: 0.2340 - val_loss: 0.0013 - val_acc: 0.2388\n",
      "Epoch 199/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0083 - acc: 0.2226 - val_loss: 3.3793e-04 - val_acc: 0.3134\n",
      "Epoch 200/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0096 - acc: 0.2302 - val_loss: 5.1103e-04 - val_acc: 0.3134\n",
      "Epoch 201/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0083 - acc: 0.2528 - val_loss: 8.0039e-04 - val_acc: 0.2239\n",
      "Epoch 202/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0089 - acc: 0.2377 - val_loss: 6.8596e-04 - val_acc: 0.2388\n",
      "Epoch 203/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0087 - acc: 0.2377 - val_loss: 8.2144e-04 - val_acc: 0.2239\n",
      "Epoch 204/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0088 - acc: 0.2679 - val_loss: 2.5894e-04 - val_acc: 0.2239\n",
      "Epoch 205/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0091 - acc: 0.2264 - val_loss: 0.0013 - val_acc: 0.2239\n",
      "Epoch 206/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0089 - acc: 0.2679 - val_loss: 2.4589e-04 - val_acc: 0.2388\n",
      "Epoch 207/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0089 - acc: 0.2528 - val_loss: 8.7335e-04 - val_acc: 0.2239\n",
      "Epoch 208/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0085 - acc: 0.2302 - val_loss: 6.3917e-04 - val_acc: 0.2239\n",
      "Epoch 209/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0080 - acc: 0.2377 - val_loss: 3.3002e-04 - val_acc: 0.2388\n",
      "Epoch 210/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0085 - acc: 0.2415 - val_loss: 5.7490e-04 - val_acc: 0.3134\n",
      "Epoch 211/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2679 - val_loss: 5.6763e-04 - val_acc: 0.2239\n",
      "Epoch 212/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0081 - acc: 0.2113 - val_loss: 7.1681e-04 - val_acc: 0.2239\n",
      "Epoch 213/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0080 - acc: 0.2792 - val_loss: 3.6327e-04 - val_acc: 0.3134\n",
      "Epoch 214/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0086 - acc: 0.2302 - val_loss: 0.0010 - val_acc: 0.3134\n",
      "Epoch 215/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0083 - acc: 0.2189 - val_loss: 6.0578e-04 - val_acc: 0.2388\n",
      "Epoch 216/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0087 - acc: 0.2792 - val_loss: 4.9767e-04 - val_acc: 0.2239\n",
      "Epoch 217/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0088 - acc: 0.2566 - val_loss: 5.2898e-04 - val_acc: 0.2388\n",
      "Epoch 218/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0088 - acc: 0.2491 - val_loss: 0.0016 - val_acc: 0.2388\n",
      "Epoch 219/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0093 - acc: 0.2528 - val_loss: 2.2178e-04 - val_acc: 0.2239\n",
      "Epoch 220/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0083 - acc: 0.2868 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 221/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0082 - acc: 0.2453 - val_loss: 2.1923e-04 - val_acc: 0.2388\n",
      "Epoch 222/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0081 - acc: 0.2453 - val_loss: 0.0012 - val_acc: 0.2388\n",
      "Epoch 223/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0078 - acc: 0.2830 - val_loss: 4.4497e-04 - val_acc: 0.2239\n",
      "Epoch 224/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0082 - acc: 0.2528 - val_loss: 5.7319e-04 - val_acc: 0.2239\n",
      "Epoch 225/500\n",
      "265/265 [==============================] - ETA: 0s - loss: 0.0085 - acc: 0.308 - 1s 3ms/step - loss: 0.0085 - acc: 0.3094 - val_loss: 4.4844e-04 - val_acc: 0.2388\n",
      "Epoch 226/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0081 - acc: 0.2566 - val_loss: 5.9887e-04 - val_acc: 0.2239\n",
      "Epoch 227/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0085 - acc: 0.2906 - val_loss: 2.2028e-04 - val_acc: 0.2239\n",
      "Epoch 228/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0083 - acc: 0.2377 - val_loss: 9.0065e-04 - val_acc: 0.2239\n",
      "Epoch 229/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0077 - acc: 0.2604 - val_loss: 3.1630e-04 - val_acc: 0.2388\n",
      "Epoch 230/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0086 - acc: 0.2302 - val_loss: 6.3478e-04 - val_acc: 0.2388\n",
      "Epoch 231/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0081 - acc: 0.2981 - val_loss: 4.4633e-04 - val_acc: 0.2239\n",
      "Epoch 232/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0078 - acc: 0.2679 - val_loss: 4.4701e-04 - val_acc: 0.2239\n",
      "Epoch 233/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0083 - acc: 0.2566 - val_loss: 6.9892e-04 - val_acc: 0.3134\n",
      "Epoch 234/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0081 - acc: 0.2679 - val_loss: 5.5826e-04 - val_acc: 0.2388\n",
      "Epoch 235/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0086 - acc: 0.2604 - val_loss: 8.0535e-04 - val_acc: 0.2388\n",
      "Epoch 236/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0074 - acc: 0.2755 - val_loss: 0.0014 - val_acc: 0.2239\n",
      "Epoch 237/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0080 - acc: 0.2981 - val_loss: 2.0789e-04 - val_acc: 0.3134\n",
      "Epoch 238/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0085 - acc: 0.2566 - val_loss: 8.3854e-04 - val_acc: 0.2239\n",
      "Epoch 239/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0083 - acc: 0.2717 - val_loss: 8.3741e-04 - val_acc: 0.2239\n",
      "Epoch 240/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0077 - acc: 0.2642 - val_loss: 3.0918e-04 - val_acc: 0.2239\n",
      "Epoch 241/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0080 - acc: 0.2038 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 242/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0075 - acc: 0.2755 - val_loss: 2.0579e-04 - val_acc: 0.3134\n",
      "Epoch 243/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0077 - acc: 0.2981 - val_loss: 9.9524e-04 - val_acc: 0.2388\n",
      "Epoch 244/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0073 - acc: 0.2528 - val_loss: 2.9137e-04 - val_acc: 0.2388\n",
      "Epoch 245/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0074 - acc: 0.2717 - val_loss: 6.0451e-04 - val_acc: 0.2239\n",
      "Epoch 246/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0076 - acc: 0.2377 - val_loss: 3.3391e-04 - val_acc: 0.2239\n",
      "Epoch 247/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0083 - acc: 0.2642 - val_loss: 8.3611e-04 - val_acc: 0.2239\n",
      "Epoch 248/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0086 - acc: 0.2302 - val_loss: 2.1452e-04 - val_acc: 0.2388\n",
      "Epoch 249/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0083 - acc: 0.2415 - val_loss: 0.0015 - val_acc: 0.2239\n",
      "Epoch 250/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0082 - acc: 0.2453 - val_loss: 2.2397e-04 - val_acc: 0.3134\n",
      "Epoch 251/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0083 - acc: 0.2453 - val_loss: 8.1222e-04 - val_acc: 0.3134\n",
      "Epoch 252/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2566 - val_loss: 3.3626e-04 - val_acc: 0.2239\n",
      "Epoch 253/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0065 - acc: 0.2302 - val_loss: 9.7180e-04 - val_acc: 0.2239\n",
      "Epoch 254/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0075 - acc: 0.2151 - val_loss: 3.6152e-04 - val_acc: 0.2239\n",
      "Epoch 255/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0074 - acc: 0.2491 - val_loss: 7.1894e-04 - val_acc: 0.3134\n",
      "Epoch 256/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0085 - acc: 0.2755 - val_loss: 4.2223e-04 - val_acc: 0.2239\n",
      "Epoch 257/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2415 - val_loss: 5.1790e-04 - val_acc: 0.2239\n",
      "Epoch 258/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0073 - acc: 0.2264 - val_loss: 5.6568e-04 - val_acc: 0.2239\n",
      "Epoch 259/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0080 - acc: 0.2755 - val_loss: 7.9888e-04 - val_acc: 0.2239\n",
      "Epoch 260/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0080 - acc: 0.2717 - val_loss: 3.6163e-04 - val_acc: 0.2239\n",
      "Epoch 261/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2151 - val_loss: 3.4668e-04 - val_acc: 0.2239\n",
      "Epoch 262/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0081 - acc: 0.2038 - val_loss: 8.5886e-04 - val_acc: 0.2388\n",
      "Epoch 263/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0075 - acc: 0.2717 - val_loss: 3.0174e-04 - val_acc: 0.2388\n",
      "Epoch 264/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0083 - acc: 0.2528 - val_loss: 5.2514e-04 - val_acc: 0.2239\n",
      "Epoch 265/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0079 - acc: 0.2038 - val_loss: 0.0014 - val_acc: 0.2239\n",
      "Epoch 266/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0080 - acc: 0.2189 - val_loss: 2.2702e-04 - val_acc: 0.2239\n",
      "Epoch 267/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0076 - acc: 0.2189 - val_loss: 9.9309e-04 - val_acc: 0.2239\n",
      "Epoch 268/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0074 - acc: 0.2679 - val_loss: 3.8558e-04 - val_acc: 0.3134\n",
      "Epoch 269/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0077 - acc: 0.1962 - val_loss: 9.9218e-04 - val_acc: 0.2239\n",
      "Epoch 270/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0077 - acc: 0.1623 - val_loss: 2.0633e-04 - val_acc: 0.3134\n",
      "Epoch 271/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0082 - acc: 0.2264 - val_loss: 0.0013 - val_acc: 0.2388\n",
      "Epoch 272/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0074 - acc: 0.2226 - val_loss: 2.4604e-04 - val_acc: 0.2388\n",
      "Epoch 273/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0074 - acc: 0.2491 - val_loss: 8.6550e-04 - val_acc: 0.2388\n",
      "Epoch 274/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2000 - val_loss: 9.2449e-04 - val_acc: 0.3134\n",
      "Epoch 275/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0070 - acc: 0.2566 - val_loss: 2.5759e-04 - val_acc: 0.2239\n",
      "Epoch 276/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0070 - acc: 0.2377 - val_loss: 9.1114e-04 - val_acc: 0.2239\n",
      "Epoch 277/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0075 - acc: 0.3057 - val_loss: 2.1993e-04 - val_acc: 0.3134\n",
      "Epoch 278/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0075 - acc: 0.2679 - val_loss: 0.0017 - val_acc: 0.2388\n",
      "Epoch 279/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0073 - acc: 0.2415 - val_loss: 2.5499e-04 - val_acc: 0.2388\n",
      "Epoch 280/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0064 - acc: 0.2302 - val_loss: 5.2591e-04 - val_acc: 0.2388\n",
      "Epoch 281/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0073 - acc: 0.2151 - val_loss: 5.8577e-04 - val_acc: 0.3134\n",
      "Epoch 282/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0070 - acc: 0.2453 - val_loss: 5.8928e-04 - val_acc: 0.2239\n",
      "Epoch 283/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0069 - acc: 0.2415 - val_loss: 7.3189e-04 - val_acc: 0.2239\n",
      "Epoch 284/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0064 - acc: 0.2264 - val_loss: 6.2318e-04 - val_acc: 0.2239\n",
      "Epoch 285/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0065 - acc: 0.2302 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 286/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0069 - acc: 0.2377 - val_loss: 4.7148e-04 - val_acc: 0.3134\n",
      "Epoch 287/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0077 - acc: 0.1811 - val_loss: 4.6218e-04 - val_acc: 0.2239\n",
      "Epoch 288/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0070 - acc: 0.2075 - val_loss: 8.9535e-04 - val_acc: 0.2239\n",
      "Epoch 289/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0068 - acc: 0.2189 - val_loss: 2.4735e-04 - val_acc: 0.2388\n",
      "Epoch 290/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2302 - val_loss: 6.3480e-04 - val_acc: 0.2388\n",
      "Epoch 291/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0066 - acc: 0.2189 - val_loss: 5.0619e-04 - val_acc: 0.2388\n",
      "Epoch 292/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0074 - acc: 0.2642 - val_loss: 2.4736e-04 - val_acc: 0.2239\n",
      "Epoch 293/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0075 - acc: 0.2755 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 294/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0063 - acc: 0.2453 - val_loss: 2.2939e-04 - val_acc: 0.3134\n",
      "Epoch 295/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0070 - acc: 0.1925 - val_loss: 9.9061e-04 - val_acc: 0.2239\n",
      "Epoch 296/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0071 - acc: 0.2830 - val_loss: 6.0567e-04 - val_acc: 0.2388\n",
      "Epoch 297/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0065 - acc: 0.2151 - val_loss: 3.4835e-04 - val_acc: 0.2388\n",
      "Epoch 298/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2566 - val_loss: 0.0010 - val_acc: 0.2239\n",
      "Epoch 299/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0068 - acc: 0.2566 - val_loss: 2.4807e-04 - val_acc: 0.2239\n",
      "Epoch 300/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0069 - acc: 0.2264 - val_loss: 8.1060e-04 - val_acc: 0.2239\n",
      "Epoch 301/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0073 - acc: 0.2453 - val_loss: 4.5946e-04 - val_acc: 0.2239\n",
      "Epoch 302/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0070 - acc: 0.2415 - val_loss: 5.4654e-04 - val_acc: 0.2239\n",
      "Epoch 303/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0066 - acc: 0.2226 - val_loss: 5.1032e-04 - val_acc: 0.3134\n",
      "Epoch 304/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2453 - val_loss: 4.5344e-04 - val_acc: 0.2239\n",
      "Epoch 305/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0069 - acc: 0.1849 - val_loss: 7.1525e-04 - val_acc: 0.2239\n",
      "Epoch 306/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0066 - acc: 0.2528 - val_loss: 3.4066e-04 - val_acc: 0.2239\n",
      "Epoch 307/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0068 - acc: 0.3057 - val_loss: 0.0010 - val_acc: 0.2388\n",
      "Epoch 308/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0072 - acc: 0.2868 - val_loss: 2.1877e-04 - val_acc: 0.3134\n",
      "Epoch 309/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0070 - acc: 0.2189 - val_loss: 6.8649e-04 - val_acc: 0.2239\n",
      "Epoch 310/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0067 - acc: 0.2151 - val_loss: 3.6296e-04 - val_acc: 0.2239\n",
      "Epoch 311/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0063 - acc: 0.2415 - val_loss: 4.6079e-04 - val_acc: 0.2388\n",
      "Epoch 312/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0071 - acc: 0.2604 - val_loss: 4.7454e-04 - val_acc: 0.2239\n",
      "Epoch 313/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0063 - acc: 0.2566 - val_loss: 8.7643e-04 - val_acc: 0.2239\n",
      "Epoch 314/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0070 - acc: 0.2377 - val_loss: 3.7111e-04 - val_acc: 0.3134\n",
      "Epoch 315/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0068 - acc: 0.2340 - val_loss: 7.0008e-04 - val_acc: 0.2239\n",
      "Epoch 316/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0067 - acc: 0.2604 - val_loss: 2.9879e-04 - val_acc: 0.2388\n",
      "Epoch 317/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0071 - acc: 0.2226 - val_loss: 5.0677e-04 - val_acc: 0.2239\n",
      "Epoch 318/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0061 - acc: 0.2264 - val_loss: 5.3578e-04 - val_acc: 0.2239\n",
      "Epoch 319/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0063 - acc: 0.2566 - val_loss: 3.8011e-04 - val_acc: 0.3134\n",
      "Epoch 320/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0062 - acc: 0.2226 - val_loss: 4.8565e-04 - val_acc: 0.3134\n",
      "Epoch 321/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0068 - acc: 0.2604 - val_loss: 4.2035e-04 - val_acc: 0.3134\n",
      "Epoch 322/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0068 - acc: 0.2642 - val_loss: 7.6189e-04 - val_acc: 0.2388\n",
      "Epoch 323/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2377 - val_loss: 6.0783e-04 - val_acc: 0.3134\n",
      "Epoch 324/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0064 - acc: 0.2377 - val_loss: 4.4785e-04 - val_acc: 0.2239\n",
      "Epoch 325/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0065 - acc: 0.2566 - val_loss: 8.6890e-04 - val_acc: 0.2388\n",
      "Epoch 326/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0060 - acc: 0.2189 - val_loss: 3.8990e-04 - val_acc: 0.2388\n",
      "Epoch 327/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0066 - acc: 0.2415 - val_loss: 7.5703e-04 - val_acc: 0.2239\n",
      "Epoch 328/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2717 - val_loss: 4.1614e-04 - val_acc: 0.2388\n",
      "Epoch 329/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0059 - acc: 0.2453 - val_loss: 6.9597e-04 - val_acc: 0.2239\n",
      "Epoch 330/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0061 - acc: 0.2792 - val_loss: 2.3120e-04 - val_acc: 0.2239\n",
      "Epoch 331/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2075 - val_loss: 9.6466e-04 - val_acc: 0.2239\n",
      "Epoch 332/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0061 - acc: 0.2528 - val_loss: 3.3282e-04 - val_acc: 0.2388\n",
      "Epoch 333/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0069 - acc: 0.2604 - val_loss: 6.5508e-04 - val_acc: 0.2239\n",
      "Epoch 334/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0062 - acc: 0.2302 - val_loss: 6.4856e-04 - val_acc: 0.3134\n",
      "Epoch 335/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0060 - acc: 0.2302 - val_loss: 4.0143e-04 - val_acc: 0.3134\n",
      "Epoch 336/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0063 - acc: 0.2226 - val_loss: 6.5657e-04 - val_acc: 0.2388\n",
      "Epoch 337/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0065 - acc: 0.2491 - val_loss: 5.6703e-04 - val_acc: 0.2388\n",
      "Epoch 338/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0063 - acc: 0.2302 - val_loss: 4.3320e-04 - val_acc: 0.2388\n",
      "Epoch 339/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0065 - acc: 0.2566 - val_loss: 5.6084e-04 - val_acc: 0.2239\n",
      "Epoch 340/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0061 - acc: 0.2226 - val_loss: 6.0315e-04 - val_acc: 0.2239\n",
      "Epoch 341/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0058 - acc: 0.2113 - val_loss: 6.4764e-04 - val_acc: 0.2239\n",
      "Epoch 342/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0065 - acc: 0.2642 - val_loss: 5.2655e-04 - val_acc: 0.2239\n",
      "Epoch 343/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 0.2792 - val_loss: 3.8671e-04 - val_acc: 0.2239\n",
      "Epoch 344/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0065 - acc: 0.2377 - val_loss: 4.6061e-04 - val_acc: 0.3134\n",
      "Epoch 345/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 0.2755 - val_loss: 5.1153e-04 - val_acc: 0.2388\n",
      "Epoch 346/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0062 - acc: 0.2302 - val_loss: 4.8996e-04 - val_acc: 0.2388\n",
      "Epoch 347/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0058 - acc: 0.1849 - val_loss: 4.5345e-04 - val_acc: 0.2388\n",
      "Epoch 348/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0058 - acc: 0.2000 - val_loss: 3.5063e-04 - val_acc: 0.2239\n",
      "Epoch 349/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 0.2981 - val_loss: 4.2264e-04 - val_acc: 0.2239\n",
      "Epoch 350/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0062 - acc: 0.1925 - val_loss: 6.7906e-04 - val_acc: 0.3134\n",
      "Epoch 351/500\n",
      "265/265 [==============================] - ETA: 0s - loss: 0.0058 - acc: 0.253 - 1s 3ms/step - loss: 0.0058 - acc: 0.2491 - val_loss: 4.2232e-04 - val_acc: 0.2388\n",
      "Epoch 352/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0062 - acc: 0.2151 - val_loss: 5.8296e-04 - val_acc: 0.2388\n",
      "Epoch 353/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.3057 - val_loss: 7.5017e-04 - val_acc: 0.2388\n",
      "Epoch 354/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0063 - acc: 0.2415 - val_loss: 2.2498e-04 - val_acc: 0.3134\n",
      "Epoch 355/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0065 - acc: 0.2642 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 356/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0062 - acc: 0.2755 - val_loss: 2.6244e-04 - val_acc: 0.2388\n",
      "Epoch 357/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0064 - acc: 0.2453 - val_loss: 0.0011 - val_acc: 0.2388\n",
      "Epoch 358/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2264 - val_loss: 2.1138e-04 - val_acc: 0.2388\n",
      "Epoch 359/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0059 - acc: 0.2453 - val_loss: 8.9755e-04 - val_acc: 0.2239\n",
      "Epoch 360/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0063 - acc: 0.2642 - val_loss: 2.0156e-04 - val_acc: 0.2388\n",
      "Epoch 361/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0062 - acc: 0.2302 - val_loss: 0.0011 - val_acc: 0.2388\n",
      "Epoch 362/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0062 - acc: 0.2491 - val_loss: 3.6330e-04 - val_acc: 0.2388\n",
      "Epoch 363/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0061 - acc: 0.2566 - val_loss: 9.7695e-04 - val_acc: 0.2388\n",
      "Epoch 364/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 0.2642 - val_loss: 2.1517e-04 - val_acc: 0.2239\n",
      "Epoch 365/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2415 - val_loss: 0.0014 - val_acc: 0.2239\n",
      "Epoch 366/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2491 - val_loss: 2.8750e-04 - val_acc: 0.2388\n",
      "Epoch 367/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0061 - acc: 0.2226 - val_loss: 9.3621e-04 - val_acc: 0.2239\n",
      "Epoch 368/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 0.2755 - val_loss: 3.7859e-04 - val_acc: 0.2239\n",
      "Epoch 369/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0056 - acc: 0.2755 - val_loss: 6.8518e-04 - val_acc: 0.2239\n",
      "Epoch 370/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0062 - acc: 0.3358 - val_loss: 6.9993e-04 - val_acc: 0.2388\n",
      "Epoch 371/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0057 - acc: 0.2377 - val_loss: 7.2087e-04 - val_acc: 0.2388\n",
      "Epoch 372/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0054 - acc: 0.2981 - val_loss: 2.8234e-04 - val_acc: 0.2388\n",
      "Epoch 373/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0058 - acc: 0.2604 - val_loss: 7.8332e-04 - val_acc: 0.2239\n",
      "Epoch 374/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0059 - acc: 0.1849 - val_loss: 4.1211e-04 - val_acc: 0.2239\n",
      "Epoch 375/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0058 - acc: 0.2604 - val_loss: 0.0012 - val_acc: 0.2239\n",
      "Epoch 376/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0056 - acc: 0.2566 - val_loss: 4.4963e-04 - val_acc: 0.3134\n",
      "Epoch 377/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0063 - acc: 0.2189 - val_loss: 4.6654e-04 - val_acc: 0.2388\n",
      "Epoch 378/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0056 - acc: 0.2528 - val_loss: 4.8199e-04 - val_acc: 0.2239\n",
      "Epoch 379/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2038 - val_loss: 6.3315e-04 - val_acc: 0.2239\n",
      "Epoch 380/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2151 - val_loss: 3.9266e-04 - val_acc: 0.2388\n",
      "Epoch 381/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0058 - acc: 0.2868 - val_loss: 4.1727e-04 - val_acc: 0.2239\n",
      "Epoch 382/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0054 - acc: 0.2189 - val_loss: 4.5140e-04 - val_acc: 0.2239\n",
      "Epoch 383/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2453 - val_loss: 7.2791e-04 - val_acc: 0.3134\n",
      "Epoch 384/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0056 - acc: 0.2830 - val_loss: 5.1571e-04 - val_acc: 0.2239\n",
      "Epoch 385/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0056 - acc: 0.2000 - val_loss: 3.2643e-04 - val_acc: 0.2388\n",
      "Epoch 386/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0057 - acc: 0.2189 - val_loss: 7.5147e-04 - val_acc: 0.2239\n",
      "Epoch 387/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2642 - val_loss: 4.4710e-04 - val_acc: 0.3134\n",
      "Epoch 388/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0055 - acc: 0.2226 - val_loss: 5.2762e-04 - val_acc: 0.2239\n",
      "Epoch 389/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0055 - acc: 0.2679 - val_loss: 7.6358e-04 - val_acc: 0.3134\n",
      "Epoch 390/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2113 - val_loss: 5.0218e-04 - val_acc: 0.2388\n",
      "Epoch 391/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2830 - val_loss: 4.8466e-04 - val_acc: 0.3134\n",
      "Epoch 392/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2830 - val_loss: 3.3234e-04 - val_acc: 0.2239\n",
      "Epoch 393/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2717 - val_loss: 8.6724e-04 - val_acc: 0.2239\n",
      "Epoch 394/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0053 - acc: 0.2679 - val_loss: 6.2493e-04 - val_acc: 0.2239\n",
      "Epoch 395/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2679 - val_loss: 5.8071e-04 - val_acc: 0.2239\n",
      "Epoch 396/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2981 - val_loss: 6.7071e-04 - val_acc: 0.2239\n",
      "Epoch 397/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2604 - val_loss: 4.0316e-04 - val_acc: 0.2239\n",
      "Epoch 398/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2453 - val_loss: 8.8122e-04 - val_acc: 0.2239\n",
      "Epoch 399/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0060 - acc: 0.2189 - val_loss: 3.5511e-04 - val_acc: 0.3134\n",
      "Epoch 400/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2038 - val_loss: 4.9145e-04 - val_acc: 0.2388\n",
      "Epoch 401/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0057 - acc: 0.2189 - val_loss: 4.3064e-04 - val_acc: 0.2388\n",
      "Epoch 402/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2566 - val_loss: 5.6453e-04 - val_acc: 0.2388\n",
      "Epoch 403/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0054 - acc: 0.2075 - val_loss: 7.2129e-04 - val_acc: 0.2239\n",
      "Epoch 404/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0056 - acc: 0.2264 - val_loss: 3.1693e-04 - val_acc: 0.3134\n",
      "Epoch 405/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0055 - acc: 0.2415 - val_loss: 9.6936e-04 - val_acc: 0.2388\n",
      "Epoch 406/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0056 - acc: 0.2528 - val_loss: 3.1564e-04 - val_acc: 0.2388\n",
      "Epoch 407/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0058 - acc: 0.3019 - val_loss: 7.4201e-04 - val_acc: 0.2388\n",
      "Epoch 408/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0051 - acc: 0.2792 - val_loss: 4.8288e-04 - val_acc: 0.3134\n",
      "Epoch 409/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2302 - val_loss: 3.3508e-04 - val_acc: 0.3134\n",
      "Epoch 410/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0053 - acc: 0.2264 - val_loss: 8.5388e-04 - val_acc: 0.2239\n",
      "Epoch 411/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0054 - acc: 0.2943 - val_loss: 2.8369e-04 - val_acc: 0.2239\n",
      "Epoch 412/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0055 - acc: 0.1849 - val_loss: 6.8670e-04 - val_acc: 0.2239\n",
      "Epoch 413/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0053 - acc: 0.2415 - val_loss: 5.9167e-04 - val_acc: 0.2388\n",
      "Epoch 414/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2377 - val_loss: 5.0873e-04 - val_acc: 0.2388\n",
      "Epoch 415/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2830 - val_loss: 6.7634e-04 - val_acc: 0.3134\n",
      "Epoch 416/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0054 - acc: 0.3057 - val_loss: 4.7677e-04 - val_acc: 0.3134\n",
      "Epoch 417/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0054 - acc: 0.2566 - val_loss: 5.8098e-04 - val_acc: 0.3134\n",
      "Epoch 418/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0050 - acc: 0.2717 - val_loss: 3.9459e-04 - val_acc: 0.2388\n",
      "Epoch 419/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2906 - val_loss: 3.7221e-04 - val_acc: 0.2388\n",
      "Epoch 420/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0052 - acc: 0.2491 - val_loss: 4.2829e-04 - val_acc: 0.2239\n",
      "Epoch 421/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2340 - val_loss: 6.4790e-04 - val_acc: 0.3134\n",
      "Epoch 422/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2302 - val_loss: 3.7300e-04 - val_acc: 0.3134\n",
      "Epoch 423/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2604 - val_loss: 9.1129e-04 - val_acc: 0.2239\n",
      "Epoch 424/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0052 - acc: 0.3094 - val_loss: 5.2220e-04 - val_acc: 0.2388\n",
      "Epoch 425/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2226 - val_loss: 4.3997e-04 - val_acc: 0.3134\n",
      "Epoch 426/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2792 - val_loss: 7.3681e-04 - val_acc: 0.2687\n",
      "Epoch 427/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0054 - acc: 0.2642 - val_loss: 6.5081e-04 - val_acc: 0.2239\n",
      "Epoch 428/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0054 - acc: 0.2491 - val_loss: 2.8614e-04 - val_acc: 0.2388\n",
      "Epoch 429/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2415 - val_loss: 0.0011 - val_acc: 0.2239\n",
      "Epoch 430/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0058 - acc: 0.2226 - val_loss: 3.5731e-04 - val_acc: 0.2239\n",
      "Epoch 431/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0050 - acc: 0.2604 - val_loss: 7.2048e-04 - val_acc: 0.2239\n",
      "Epoch 432/500\n",
      "265/265 [==============================] - ETA: 0s - loss: 0.0050 - acc: 0.261 - 1s 2ms/step - loss: 0.0049 - acc: 0.2642 - val_loss: 3.1326e-04 - val_acc: 0.2388\n",
      "Epoch 433/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2302 - val_loss: 5.2179e-04 - val_acc: 0.3134\n",
      "Epoch 434/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2264 - val_loss: 3.9402e-04 - val_acc: 0.2388\n",
      "Epoch 435/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2755 - val_loss: 8.1527e-04 - val_acc: 0.2388\n",
      "Epoch 436/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2113 - val_loss: 4.5850e-04 - val_acc: 0.3134\n",
      "Epoch 437/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2377 - val_loss: 6.8921e-04 - val_acc: 0.2239\n",
      "Epoch 438/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2000 - val_loss: 2.3583e-04 - val_acc: 0.2239\n",
      "Epoch 439/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0054 - acc: 0.2453 - val_loss: 6.3983e-04 - val_acc: 0.2239\n",
      "Epoch 440/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2491 - val_loss: 4.9496e-04 - val_acc: 0.2388\n",
      "Epoch 441/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2415 - val_loss: 2.2699e-04 - val_acc: 0.2388\n",
      "Epoch 442/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0054 - acc: 0.2528 - val_loss: 0.0010 - val_acc: 0.3134\n",
      "Epoch 443/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0052 - acc: 0.2604 - val_loss: 2.0082e-04 - val_acc: 0.3134\n",
      "Epoch 444/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0055 - acc: 0.2491 - val_loss: 9.1731e-04 - val_acc: 0.2388\n",
      "Epoch 445/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2264 - val_loss: 3.3958e-04 - val_acc: 0.2239\n",
      "Epoch 446/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2491 - val_loss: 6.7851e-04 - val_acc: 0.2239\n",
      "Epoch 447/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0052 - acc: 0.2226 - val_loss: 5.4473e-04 - val_acc: 0.2239\n",
      "Epoch 448/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2642 - val_loss: 3.5595e-04 - val_acc: 0.2239\n",
      "Epoch 449/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0050 - acc: 0.2755 - val_loss: 7.4736e-04 - val_acc: 0.2239\n",
      "Epoch 450/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0054 - acc: 0.2113 - val_loss: 2.2532e-04 - val_acc: 0.2388\n",
      "Epoch 451/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2792 - val_loss: 5.8473e-04 - val_acc: 0.2388\n",
      "Epoch 452/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0050 - acc: 0.2642 - val_loss: 5.0271e-04 - val_acc: 0.2388\n",
      "Epoch 453/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2604 - val_loss: 3.5490e-04 - val_acc: 0.2388\n",
      "Epoch 454/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2755 - val_loss: 4.1590e-04 - val_acc: 0.2239\n",
      "Epoch 455/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2075 - val_loss: 5.0469e-04 - val_acc: 0.2239\n",
      "Epoch 456/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2340 - val_loss: 4.8486e-04 - val_acc: 0.2239\n",
      "Epoch 457/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 0.2415 - val_loss: 6.1420e-04 - val_acc: 0.2239\n",
      "Epoch 458/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2075 - val_loss: 4.4429e-04 - val_acc: 0.2388\n",
      "Epoch 459/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2491 - val_loss: 6.0052e-04 - val_acc: 0.2687\n",
      "Epoch 460/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2717 - val_loss: 4.6195e-04 - val_acc: 0.2388\n",
      "Epoch 461/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 0.2377 - val_loss: 5.7158e-04 - val_acc: 0.2239\n",
      "Epoch 462/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2189 - val_loss: 3.4107e-04 - val_acc: 0.2239\n",
      "Epoch 463/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2302 - val_loss: 4.2989e-04 - val_acc: 0.2239\n",
      "Epoch 464/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0049 - acc: 0.2189 - val_loss: 8.2269e-04 - val_acc: 0.2388\n",
      "Epoch 465/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2528 - val_loss: 4.6767e-04 - val_acc: 0.3134\n",
      "Epoch 466/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.2113 - val_loss: 6.8166e-04 - val_acc: 0.3134\n",
      "Epoch 467/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2453 - val_loss: 4.5610e-04 - val_acc: 0.2090\n",
      "Epoch 468/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 0.2377 - val_loss: 5.3357e-04 - val_acc: 0.2239\n",
      "Epoch 469/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.3170 - val_loss: 3.4111e-04 - val_acc: 0.3134\n",
      "Epoch 470/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.3208 - val_loss: 4.8603e-04 - val_acc: 0.2239\n",
      "Epoch 471/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2679 - val_loss: 5.3418e-04 - val_acc: 0.2239\n",
      "Epoch 472/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2302 - val_loss: 7.6017e-04 - val_acc: 0.2388\n",
      "Epoch 473/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2943 - val_loss: 4.1409e-04 - val_acc: 0.2388\n",
      "Epoch 474/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0049 - acc: 0.2528 - val_loss: 4.1108e-04 - val_acc: 0.2388\n",
      "Epoch 475/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2302 - val_loss: 8.2690e-04 - val_acc: 0.2239\n",
      "Epoch 476/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2755 - val_loss: 4.2749e-04 - val_acc: 0.2239\n",
      "Epoch 477/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0047 - acc: 0.2642 - val_loss: 5.1230e-04 - val_acc: 0.2388\n",
      "Epoch 478/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2151 - val_loss: 4.7453e-04 - val_acc: 0.2388\n",
      "Epoch 479/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0051 - acc: 0.2981 - val_loss: 5.7767e-04 - val_acc: 0.3134\n",
      "Epoch 480/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0051 - acc: 0.2566 - val_loss: 4.9789e-04 - val_acc: 0.2239\n",
      "Epoch 481/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0050 - acc: 0.2906 - val_loss: 6.1579e-04 - val_acc: 0.2388\n",
      "Epoch 482/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2679 - val_loss: 3.2123e-04 - val_acc: 0.2388\n",
      "Epoch 483/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2226 - val_loss: 6.4523e-04 - val_acc: 0.2388\n",
      "Epoch 484/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 0.2717 - val_loss: 7.9459e-04 - val_acc: 0.2239\n",
      "Epoch 485/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2226 - val_loss: 3.4366e-04 - val_acc: 0.2388\n",
      "Epoch 486/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.2377 - val_loss: 8.0541e-04 - val_acc: 0.2239\n",
      "Epoch 487/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.2340 - val_loss: 4.2298e-04 - val_acc: 0.2239\n",
      "Epoch 488/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.2566 - val_loss: 3.2091e-04 - val_acc: 0.2239\n",
      "Epoch 489/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0049 - acc: 0.2868 - val_loss: 5.4147e-04 - val_acc: 0.2239\n",
      "Epoch 490/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.3170 - val_loss: 4.7207e-04 - val_acc: 0.2239\n",
      "Epoch 491/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.2679 - val_loss: 4.6631e-04 - val_acc: 0.3134\n",
      "Epoch 492/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.2453 - val_loss: 8.9640e-04 - val_acc: 0.2239\n",
      "Epoch 493/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.2566 - val_loss: 2.6260e-04 - val_acc: 0.2388\n",
      "Epoch 494/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 0.2189 - val_loss: 7.2465e-04 - val_acc: 0.2239\n",
      "Epoch 495/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0047 - acc: 0.2075 - val_loss: 3.4325e-04 - val_acc: 0.2388\n",
      "Epoch 496/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0046 - acc: 0.2604 - val_loss: 5.7496e-04 - val_acc: 0.2239\n",
      "Epoch 497/500\n",
      "265/265 [==============================] - 1s 3ms/step - loss: 0.0046 - acc: 0.2302 - val_loss: 6.7912e-04 - val_acc: 0.2388\n",
      "Epoch 498/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0047 - acc: 0.3321 - val_loss: 2.0070e-04 - val_acc: 0.2388\n",
      "Epoch 499/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0046 - acc: 0.2566 - val_loss: 8.7931e-04 - val_acc: 0.2687\n",
      "Epoch 500/500\n",
      "265/265 [==============================] - 1s 2ms/step - loss: 0.0048 - acc: 0.2642 - val_loss: 2.0836e-04 - val_acc: 0.2388\n",
      "[0.6031097315392404, 0.08656559825505851, 0.04653020447717523, 0.03963575472809234, 0.03289970150393135, 0.03267110652237568, 0.029113413309151272, 0.025413883885122696, 0.025765206109521523, 0.02822371151227996, 0.024107403069172265, 0.024487144535161414, 0.02533897140397216, 0.024411440352505108, 0.02581206018913467, 0.02287424234849102, 0.022904523686980302, 0.021212334242069496, 0.020239935562295735, 0.020532863088092716, 0.021032995519773015, 0.01962515836897886, 0.019377143905972536, 0.019926995892991434, 0.021211995694013136, 0.023845443726991707, 0.019030586967490753, 0.01934613504991779, 0.01934121627852602, 0.020465700641133875, 0.017742541467524926, 0.016757048652419504, 0.0184874721363468, 0.01902784472507126, 0.018263556080747326, 0.01711133024743143, 0.018416865015367292, 0.017550148656767493, 0.018584465562313232, 0.01752159686864547, 0.015856689404485362, 0.017555750778190934, 0.01716152923884538, 0.01660727655409642, 0.01695593499912406, 0.01877403584713081, 0.016505402495276253, 0.01675461601676806, 0.016917548412984273, 0.01623333841562271, 0.01705299823913934, 0.016364849687395794, 0.01496291763307351, 0.01688981368294302, 0.015093272446461444, 0.015583489786060351, 0.01529656237799604, 0.015230084351211225, 0.015450976665992782, 0.016729545336708708, 0.014862901675251295, 0.0172285069096482, 0.01633198061641657, 0.016174750114386936, 0.014132278106825531, 0.015007940324831683, 0.014676722016115234, 0.015218076521073872, 0.01321328801788249, 0.015099128068618054, 0.013054495747640447, 0.01522736687142894, 0.012389682029496949, 0.014110995569037941, 0.014417551638874804, 0.012027086602207625, 0.013774504438745526, 0.014893152083287824, 0.014291891420506082, 0.01345520821874434, 0.01341660191076544, 0.013058304941316821, 0.01439083033012894, 0.012184687148568765, 0.013681441176471845, 0.013067055690401005, 0.013367680955748513, 0.014153368345830801, 0.013485670240841947, 0.012261468743167395, 0.013496680775622152, 0.013592300922522005, 0.013149023006828325, 0.014145421927337939, 0.012840632537274427, 0.013230567460633674, 0.014751267573743496, 0.012530336096742244, 0.011639990755971873, 0.012010758658343891, 0.011589158435334574, 0.013842650608352896, 0.011771916176350612, 0.011922905927980846, 0.013162064684098059, 0.014100740393096545, 0.012405147196127559, 0.011803655431799169, 0.012353217060554702, 0.011867256956350691, 0.013350236426406311, 0.011102743374781227, 0.01155039333509949, 0.014063999875677082, 0.01135075700311166, 0.011834193379530367, 0.011342867351365539, 0.010877322588326795, 0.011753398615796612, 0.01227760282608698, 0.01204085703587757, 0.011696163361083787, 0.011931005440090063, 0.011287628219656225, 0.011313703188778095, 0.011551810864288851, 0.010852091745385584, 0.012269195617299597, 0.010659508020529207, 0.010510712306735651, 0.010546811002324212, 0.010937965053291816, 0.01010764961675653, 0.010735937877435167, 0.010888746503810838, 0.01112413959366814, 0.011625284483691432, 0.010393436636143135, 0.011138551675204961, 0.011218390420500962, 0.010565504512556318, 0.010001711069412951, 0.011539431689482816, 0.010608679548187077, 0.010778956347197857, 0.010600818349224217, 0.011452247592497546, 0.01069054794937091, 0.010213319089193389, 0.010191110908141676, 0.011403419035223294, 0.010203816177639758, 0.010133569694633754, 0.010812143751261931, 0.009189010374160167, 0.011523200705085161, 0.010480076940905936, 0.010286886940868396, 0.010799468143509244, 0.009706140910538862, 0.010114346321601913, 0.00993431585858453, 0.010081568585691165, 0.009855825087617872, 0.009391946370168677, 0.009276258629166855, 0.010090744889007424, 0.009484411001135156, 0.009332093930328793, 0.01118096740178342, 0.009791391409652413, 0.00900514844348127, 0.009382880096025063, 0.010609190452422174, 0.010119474078265002, 0.009824815130669554, 0.010106132734777793, 0.008865723641202698, 0.009957342317222425, 0.011140664687977647, 0.009722778680822197, 0.01002853392922091, 0.009034040726650998, 0.010533469324966646, 0.00910619445988592, 0.00891540878505077, 0.008560516680854391, 0.009101602466741823, 0.009270451318050894, 0.008965877500661421, 0.00890621866609128, 0.009481122745376713, 0.009490947804923327, 0.007988794267458736, 0.008207988166162429, 0.008811601678366369, 0.008485464792136313, 0.009081661471007568, 0.008342490085172204, 0.009568431591263638, 0.008296140204271617, 0.008862002432908652, 0.008700835578284173, 0.008845072517276936, 0.009117347632868672, 0.008891531716118443, 0.00891291524441737, 0.008543100505012948, 0.00804461960698355, 0.008499447005045301, 0.007173678974779147, 0.008070922576172172, 0.007989911299269155, 0.008614910623269542, 0.008310797776288863, 0.008733871298016243, 0.008827665174063647, 0.008810151350807469, 0.009283875029990977, 0.008320465810456366, 0.008238511279506503, 0.00812867406650253, 0.00776381225869903, 0.008200344392362068, 0.008460025361053787, 0.008055822366743156, 0.008457037361936186, 0.008299291147938315, 0.0076923245212660646, 0.008631624306007377, 0.008090360160425024, 0.007783888845335481, 0.008331885888188515, 0.008149779909553956, 0.008620768462149602, 0.007408476518115627, 0.008018154903965177, 0.00853889470673957, 0.008338297437876462, 0.007685754449932643, 0.008016229915555636, 0.007491434673901717, 0.007742025696162907, 0.007299922541978787, 0.007405529028015598, 0.007559030938823268, 0.008321217916695014, 0.008603037074032538, 0.00834145424887538, 0.008225231276789926, 0.008258602367538326, 0.007179643156639529, 0.006533925922341504, 0.0075459538712957, 0.007436723989557545, 0.008460351442566738, 0.00722713967750095, 0.007267364727790064, 0.008027307632959113, 0.007957371387560412, 0.007212091363825888, 0.008122846939302278, 0.007463974312369554, 0.008290171082887167, 0.007905060924449057, 0.00799891361088123, 0.007616756322248927, 0.007359948476671048, 0.007668149901799998, 0.007696446082291176, 0.008248074756900094, 0.007387372795140968, 0.007413822549553412, 0.007159900528220636, 0.006985211008632521, 0.006954562470738618, 0.007463937152598826, 0.007454102391482524, 0.0073224901537990795, 0.0064424895980166935, 0.007293977960944176, 0.00695107855289331, 0.006930623131470298, 0.0064369281059798765, 0.006537676121125806, 0.0069189717711986235, 0.007681840150353481, 0.0070328578221137234, 0.006822100511910218, 0.007161021371424761, 0.0066331146530947595, 0.0073548129917878026, 0.007539811192677831, 0.0063309753243372124, 0.00697923478635274, 0.007101507899615, 0.006492253933716917, 0.005991855833486145, 0.0067671279352650325, 0.00685155671704912, 0.0073070613474075525, 0.0069911337123726896, 0.006644103673444885, 0.007178754352454869, 0.006867057016505948, 0.006629952899057347, 0.006786839003270527, 0.007170469624007929, 0.007019734548685967, 0.006742771105173061, 0.006339459766124217, 0.007050168805948968, 0.006279514780935814, 0.007025732008634873, 0.006782940155738648, 0.0067389444237188345, 0.007133078322095691, 0.0061460892265697694, 0.006338534451458814, 0.006193448107180028, 0.006771777647564996, 0.0067683256178532, 0.0059999490279774624, 0.006401432136882026, 0.006540990898491077, 0.005978226644988611, 0.006573133824287721, 0.006048169626661067, 0.005852948163442735, 0.006051738941075527, 0.005963553108696668, 0.006095499960916503, 0.006852489282851512, 0.006193232736638132, 0.005988117284580784, 0.006281354471619399, 0.0065074891563645515, 0.0063056266532754, 0.006452458455526042, 0.0060960609544912035, 0.005835816088431286, 0.00648105913193299, 0.005872866107186057, 0.006483729921703069, 0.005870359743295132, 0.006186336746333905, 0.005766414557495769, 0.005807030592816618, 0.005944122696506246, 0.00623524876680436, 0.0057680126631034995, 0.006229550354534163, 0.005730215959988956, 0.0062995730424827, 0.006517280752436732, 0.006169456095909172, 0.006357712040321445, 0.006007304404563499, 0.005894307926393835, 0.006277371718073791, 0.0062410205374208265, 0.006150347527713989, 0.0060773962985653926, 0.00590471840938026, 0.005740184336022105, 0.006021130612155177, 0.006082877295337758, 0.0058613948308739745, 0.005550494677615616, 0.006194225143430368, 0.005681543372448463, 0.0054385421121106396, 0.0057899928599033715, 0.005883174400144028, 0.00584415588264336, 0.005648392194196722, 0.0062528601133721474, 0.005554828583702164, 0.005732270267370315, 0.0057337193828919586, 0.005757637411127534, 0.005353603687769962, 0.0057472250239818164, 0.005611526616409702, 0.0055505788559972674, 0.005696853705822438, 0.005503206616619004, 0.005531683297089811, 0.005472463770013935, 0.005486676147875359, 0.005696687813988834, 0.00600504620563028, 0.0059635775392207335, 0.005255553388159792, 0.005128339025025027, 0.005721247041562818, 0.005726102386372832, 0.0056572017088567595, 0.006017556638052722, 0.00551993048459924, 0.005677899149826394, 0.005549315284094158, 0.005445667124300633, 0.0055739679980039035, 0.005490617265151638, 0.005587019722135562, 0.005772010125395543, 0.005106530693482678, 0.005509950497345823, 0.00531217891081535, 0.005389392423500606, 0.0054886639135766704, 0.0052931232799617745, 0.0054603683920401444, 0.005150972292670664, 0.005447980036677898, 0.005414516485366478, 0.004999119439821269, 0.0052499523895950815, 0.005226606629648299, 0.005225129678564252, 0.005526830629392896, 0.005486033203185729, 0.005181457197188206, 0.005074156194327856, 0.005151300094933864, 0.005388793031869562, 0.005368195036601908, 0.005067936668747965, 0.005811220059557906, 0.004975140758104763, 0.004947103009084766, 0.004962314552416638, 0.005007405792591426, 0.0050437495235424, 0.005204474559616087, 0.00506540720860632, 0.005186497486407324, 0.005413890282078734, 0.005020976503256638, 0.005194188492759219, 0.005360589705337331, 0.005234812193040578, 0.0055198543336031575, 0.0050764484467195736, 0.004943030792780502, 0.0051681927879745106, 0.004782117699176284, 0.004998728519586741, 0.005406603234697063, 0.00513817678504395, 0.005004386640733705, 0.005147551134545005, 0.004791474454807786, 0.00504823505245852, 0.005006412634371235, 0.004648245613196126, 0.005052046206305331, 0.004945539050726747, 0.004882132289887248, 0.004604567939257425, 0.005056874863451944, 0.004843970918852203, 0.004898998303391602, 0.004988719405219802, 0.004656818972127336, 0.004809709923025572, 0.00461753740432268, 0.00476999985721876, 0.004678723891046039, 0.004846585106174901, 0.004750613176114506, 0.0049072716231370024, 0.004881682647567875, 0.004927087575636523, 0.0049071221296095625, 0.0046615644463531255, 0.005082848532793094, 0.0050597234760885535, 0.005110047435177104, 0.0049677308565357105, 0.00476399628854058, 0.004925032211731206, 0.00456826366453414, 0.0049093757299179175, 0.00468608824061757, 0.00467142796930242, 0.004680385665511186, 0.00492456668530995, 0.004770749985714847, 0.004722504572556266, 0.004705863779647944, 0.004674886474122276, 0.004630627603937855, 0.004689635282364797, 0.004629243247843576, 0.004592576109417626, 0.004712501649287933, 0.004570481377073898, 0.0047744694394323065]\n",
      "[0.30566037, 0.31698114, 0.24150944, 0.20754717, 0.28301886, 0.25283018, 0.26792452, 0.25660378, 0.24905661, 0.26792452, 0.27169812, 0.28679246, 0.2792453, 0.2754717, 0.25660378, 0.18490566, 0.24528302, 0.24528302, 0.21886793, 0.24528302, 0.28301886, 0.23018868, 0.23396227, 0.24528302, 0.25660378, 0.30943397, 0.25660378, 0.21886793, 0.24528302, 0.26037735, 0.26792452, 0.21886793, 0.2264151, 0.26037735, 0.28301886, 0.2264151, 0.26792452, 0.3207547, 0.29056603, 0.25660378, 0.2754717, 0.25660378, 0.2792453, 0.2792453, 0.20377359, 0.25660378, 0.27169812, 0.23773585, 0.2981132, 0.25660378, 0.25660378, 0.29433963, 0.23396227, 0.28679246, 0.26037735, 0.23396227, 0.24905661, 0.26415095, 0.24905661, 0.25660378, 0.2754717, 0.26792452, 0.25660378, 0.25283018, 0.20754717, 0.19245283, 0.23018868, 0.23773585, 0.26415095, 0.27169812, 0.23773585, 0.27169812, 0.26792452, 0.23773585, 0.2264151, 0.27169812, 0.24528302, 0.2, 0.24905661, 0.23018868, 0.21509434, 0.23396227, 0.20754717, 0.21509434, 0.24150944, 0.21509434, 0.24528302, 0.28301886, 0.2754717, 0.20754717, 0.29056603, 0.24905661, 0.25660378, 0.23018868, 0.23018868, 0.18490566, 0.21886793, 0.23018868, 0.21132076, 0.22264151, 0.23396227, 0.3018868, 0.26792452, 0.26415095, 0.25283018, 0.2754717, 0.23396227, 0.26415095, 0.25283018, 0.23773585, 0.25283018, 0.26415095, 0.26792452, 0.26037735, 0.2, 0.26415095, 0.24905661, 0.26415095, 0.26792452, 0.20754717, 0.22264151, 0.23396227, 0.3018868, 0.21132076, 0.20377359, 0.2754717, 0.26792452, 0.26792452, 0.1773585, 0.25660378, 0.26037735, 0.23773585, 0.25283018, 0.24905661, 0.24528302, 0.21132076, 0.21886793, 0.25283018, 0.26792452, 0.24905661, 0.25283018, 0.24528302, 0.23773585, 0.21886793, 0.23773585, 0.22264151, 0.26415095, 0.25283018, 0.25660378, 0.2, 0.25660378, 0.28301886, 0.29056603, 0.24150944, 0.32830188, 0.24905661, 0.27169812, 0.21886793, 0.23773585, 0.29056603, 0.26792452, 0.26415095, 0.23018868, 0.21132076, 0.25283018, 0.24905661, 0.3018868, 0.25660378, 0.20377359, 0.26415095, 0.23018868, 0.23396227, 0.20754717, 0.20377359, 0.24905661, 0.25660378, 0.26792452, 0.21886793, 0.25660378, 0.2264151, 0.26415095, 0.24150944, 0.24528302, 0.24150944, 0.2792453, 0.25283018, 0.2, 0.26415095, 0.3018868, 0.28301886, 0.21509434, 0.25660378, 0.21509434, 0.27169812, 0.21886793, 0.26792452, 0.20754717, 0.23396227, 0.22264151, 0.23018868, 0.25283018, 0.23773585, 0.23773585, 0.26792452, 0.2264151, 0.26792452, 0.25283018, 0.23018868, 0.23773585, 0.24150944, 0.26792452, 0.21132076, 0.2792453, 0.23018868, 0.21886793, 0.2792453, 0.25660378, 0.24905661, 0.25283018, 0.28679246, 0.24528302, 0.24528302, 0.28301886, 0.25283018, 0.30943397, 0.25660378, 0.29056603, 0.23773585, 0.26037735, 0.23018868, 0.2981132, 0.26792452, 0.25660378, 0.26792452, 0.26037735, 0.2754717, 0.2981132, 0.25660378, 0.27169812, 0.26415095, 0.20377359, 0.2754717, 0.2981132, 0.25283018, 0.27169812, 0.23773585, 0.26415095, 0.23018868, 0.24150944, 0.24528302, 0.24528302, 0.25660378, 0.23018868, 0.21509434, 0.24905661, 0.2754717, 0.24150944, 0.2264151, 0.2754717, 0.27169812, 0.21509434, 0.20377359, 0.27169812, 0.25283018, 0.20377359, 0.21886793, 0.21886793, 0.26792452, 0.19622642, 0.16226415, 0.2264151, 0.22264151, 0.24905661, 0.2, 0.25660378, 0.23773585, 0.30566037, 0.26792452, 0.24150944, 0.23018868, 0.21509434, 0.24528302, 0.24150944, 0.2264151, 0.23018868, 0.23773585, 0.18113208, 0.20754717, 0.21886793, 0.23018868, 0.21886793, 0.26415095, 0.2754717, 0.24528302, 0.19245283, 0.28301886, 0.21509434, 0.25660378, 0.25660378, 0.2264151, 0.24528302, 0.24150944, 0.22264151, 0.24528302, 0.18490566, 0.25283018, 0.30566037, 0.28679246, 0.21886793, 0.21509434, 0.24150944, 0.26037735, 0.25660378, 0.23773585, 0.23396227, 0.26037735, 0.22264151, 0.2264151, 0.25660378, 0.22264151, 0.26037735, 0.26415095, 0.23773585, 0.23773585, 0.25660378, 0.21886793, 0.24150944, 0.27169812, 0.24528302, 0.2792453, 0.20754717, 0.25283018, 0.26037735, 0.23018868, 0.23018868, 0.22264151, 0.24905661, 0.23018868, 0.25660378, 0.22264151, 0.21132076, 0.26415095, 0.2792453, 0.23773585, 0.2754717, 0.23018868, 0.18490566, 0.2, 0.2981132, 0.19245283, 0.24905661, 0.21509434, 0.30566037, 0.24150944, 0.26415095, 0.2754717, 0.24528302, 0.2264151, 0.24528302, 0.26415095, 0.23018868, 0.24905661, 0.25660378, 0.26415095, 0.24150944, 0.24905661, 0.22264151, 0.2754717, 0.2754717, 0.33584905, 0.23773585, 0.2981132, 0.26037735, 0.18490566, 0.26037735, 0.25660378, 0.21886793, 0.25283018, 0.20377359, 0.21509434, 0.28679246, 0.21886793, 0.24528302, 0.28301886, 0.2, 0.21886793, 0.26415095, 0.22264151, 0.26792452, 0.21132076, 0.28301886, 0.28301886, 0.27169812, 0.26792452, 0.26792452, 0.2981132, 0.26037735, 0.24528302, 0.21886793, 0.20377359, 0.21886793, 0.25660378, 0.20754717, 0.2264151, 0.24150944, 0.25283018, 0.3018868, 0.2792453, 0.23018868, 0.2264151, 0.29433963, 0.18490566, 0.24150944, 0.23773585, 0.28301886, 0.30566037, 0.25660378, 0.27169812, 0.29056603, 0.24905661, 0.23396227, 0.23018868, 0.26037735, 0.30943397, 0.22264151, 0.2792453, 0.26415095, 0.24905661, 0.24150944, 0.22264151, 0.26037735, 0.26415095, 0.23018868, 0.2264151, 0.2754717, 0.21132076, 0.23773585, 0.2, 0.24528302, 0.24905661, 0.24150944, 0.25283018, 0.26037735, 0.24905661, 0.2264151, 0.24905661, 0.22264151, 0.26415095, 0.2754717, 0.21132076, 0.2792453, 0.26415095, 0.26037735, 0.2754717, 0.20754717, 0.23396227, 0.24150944, 0.20754717, 0.24905661, 0.27169812, 0.23773585, 0.21886793, 0.23018868, 0.21886793, 0.25283018, 0.21132076, 0.24528302, 0.23773585, 0.31698114, 0.3207547, 0.26792452, 0.23018868, 0.29433963, 0.25283018, 0.23018868, 0.2754717, 0.26415095, 0.21509434, 0.2981132, 0.25660378, 0.29056603, 0.26792452, 0.22264151, 0.27169812, 0.22264151, 0.23773585, 0.23396227, 0.25660378, 0.28679246, 0.31698114, 0.26792452, 0.24528302, 0.25660378, 0.21886793, 0.20754717, 0.26037735, 0.23018868, 0.33207548, 0.25660378, 0.26415095]\n",
      "[0.06342510072820222, 0.010298411056065737, 0.0031671735159218757, 0.004536800385355505, 0.0009899728300758817, 0.0006960016447675429, 0.0013036773585255689, 0.00030578311086187503, 0.0014486211063618312, 0.00030445671986107395, 0.00035452763871428794, 0.0009214120635540405, 0.00025088933620987625, 0.00046330461679930005, 0.0018449645565906123, 0.0002686529490469482, 0.00221660750945891, 0.0003393752436994561, 0.0002840603050192012, 0.001313611727777813, 0.000659869046388333, 0.0011261544110420258, 0.0007263096614129175, 0.0007257173750063043, 0.0009085824174472867, 0.0009544532947631469, 0.0006317073807690237, 0.0004822918337269394, 0.0008660760819355943, 0.0006123258873000186, 0.00028242817393559687, 0.0009210437220006959, 0.0003294092181673857, 0.0021439581143377876, 0.0004641280041263897, 0.0004057140535251029, 0.0021150773495380113, 0.0003531412933883605, 0.0006204164150838198, 0.0012444700409811158, 0.0005676866024246078, 0.00044290514344891616, 0.00032870718555997556, 0.0005487184059361357, 0.0008903829217540906, 0.0011333170913354453, 0.0002846223566726323, 0.001161343430224528, 0.0004431764085241941, 0.0005644319384400525, 0.0008695325466679104, 0.00021659398706431793, 0.0025074343412502934, 0.00039301000966832957, 0.0015668829472096108, 0.00039533993972938006, 0.0009605630731849528, 0.0003379862577438966, 0.00048222541096689764, 0.001502077986569658, 0.0008667156190625322, 0.0003046962283134683, 0.0026272436944462024, 0.0003109489531849629, 0.002028397781385192, 0.000248289780716026, 0.0012590526968046132, 0.00031094392110009804, 0.0008518604002197954, 0.0010448592565536722, 0.00042728114301519496, 0.0012691322342256335, 0.0002695623389532122, 0.00041988087816289, 0.0005678360935981705, 0.00045720743251714244, 0.0004157756469142971, 0.000501031771045067, 0.00021776984545225, 0.0015361397996294633, 0.00024461934630543605, 0.0010564755961132138, 0.00024998555878184234, 0.0006745613324316914, 0.0009277329377051609, 0.0009519304409484143, 0.000653712057209671, 0.0009520387089352554, 0.0011832966788936016, 0.0006691843606601122, 0.00037134390145387335, 0.0011353898157518525, 0.00021974678156087732, 0.001316582625940331, 0.0006424202375225167, 0.0009286691688362564, 0.0002983525597817028, 0.0009882014206568919, 0.00032170055609599533, 0.000629794809084608, 0.0008991718337412423, 0.00023190842916643415, 0.0017868022872274047, 0.0002177567594795863, 0.002842674007528086, 0.00022034808095138862, 0.0008072471211372471, 0.00047743955797362907, 0.00040303451467805834, 0.001086581684301482, 0.0007958296172098437, 0.0006464276812039316, 0.0014289374004549055, 0.0004434876361993878, 0.0009765345029838717, 0.00022126239752257937, 0.0011456676658048335, 0.0005008162619587757, 0.000800602451668802, 0.0002223022862897018, 0.0005812692523711542, 0.001265911884091572, 0.0002874782897094126, 0.0004514442203364877, 0.0010796200063552207, 0.00035865999690479417, 0.0009894506439372007, 0.0002543510065097779, 0.0014427593273263592, 0.00022172908157061562, 0.0017220230474234073, 0.00022677136254983384, 0.0007353189276225531, 0.0005543364304006655, 0.00084434357756721, 0.00022741606365194293, 0.0012796581931301017, 0.00039137647975472485, 0.0013678897773402173, 0.00047591753623712417, 0.0008573872662747084, 0.00025009535015873446, 0.0014809307906387458, 0.00021452834465238613, 0.0018563539609869025, 0.00023914160860229785, 0.0009645307801469271, 0.00039561272993225103, 0.0005862647667527199, 0.0006422031550918386, 0.0006105933388792424, 0.00028466407006342355, 0.0005380320006550582, 0.0004448195339055426, 0.00031339976399678235, 0.0017996089873529638, 0.00021805163080206336, 0.0009080307234996068, 0.0010557063219290394, 0.0005994506889203591, 0.00036405875224189194, 0.0006885853406639575, 0.0006776049866952669, 0.00036882094773046893, 0.0009004716889988353, 0.00034670999018239105, 0.0008151577941989943, 0.0006712663566012547, 0.0006230083891927307, 0.0011299936354521717, 0.00027475151891034763, 0.0004461677388788493, 0.0011112949786596556, 0.00025253951242853843, 0.0015001849274832145, 0.00032635852946455023, 0.0005840459802021175, 0.00022909483709893842, 0.0012259992923754365, 0.0002599765805948053, 0.0007662844163045954, 0.00034076646109347915, 0.0009215109436008245, 0.0003929025560457593, 0.000756640763042856, 0.00038493534336353085, 0.00041677351778637227, 0.0008386393293126751, 0.0005431748099569509, 0.0006469847406816683, 0.0006368177023771873, 0.0008731394443216164, 0.000596936217877569, 0.0007001757682579111, 0.0002841663133325194, 0.0015747887586285152, 0.0003136224191594146, 0.0013009074800856301, 0.0003379259619917443, 0.0005110267422912615, 0.0008003921248018742, 0.0006859559824676322, 0.000821440313845428, 0.0002589356256284014, 0.0013244344449754972, 0.0002458881681920516, 0.000873346866886896, 0.0006391705411708733, 0.00033001621199924664, 0.00057489638400973, 0.0005676302469835909, 0.0007168132700582049, 0.00036327254744733116, 0.0010476137612209615, 0.0006057839361088935, 0.0004976732906565737, 0.0005289813305543208, 0.0016052423552401474, 0.00022178046698960018, 0.0011623830312112375, 0.00021923278288354527, 0.001162002737105671, 0.00044496544400480256, 0.000573190135026434, 0.00044844247167930007, 0.0005988672717974813, 0.00022028425231457812, 0.0009006494169693385, 0.0003162997191834416, 0.0006347789547281034, 0.0004463292660066552, 0.00044701288457709684, 0.0006989221046887227, 0.0005582559396818614, 0.0008053515165516022, 0.0014008798952728733, 0.0002078904276526943, 0.0008385378325274631, 0.0008374090765171976, 0.00030917611013083206, 0.001180168880677935, 0.00020579200971232198, 0.0009952392707120127, 0.00029137124065935276, 0.0006045087061669511, 0.00033390548022867026, 0.0008361115947421362, 0.0002145184091693694, 0.0015061691337362377, 0.00022397236985418557, 0.0008122174524521428, 0.00033625671744291015, 0.0009718002723668938, 0.0003615219833211167, 0.0007189382206467884, 0.0004222281429214653, 0.0005178971071866577, 0.0005656754200124363, 0.0007988838704803319, 0.000361632841114483, 0.0003466796435749353, 0.0008588628652297072, 0.0003017407729908991, 0.000525139745693209, 0.0013814606884522225, 0.0002270203330050181, 0.000993086426491057, 0.00038558264860688753, 0.0009921762411162925, 0.00020633483991468215, 0.001319561021597083, 0.0002460371785008335, 0.0008654957150679027, 0.0009244902062910929, 0.0002575880420435943, 0.0009111377911002779, 0.00021992873341481918, 0.0017393834492775487, 0.00025499228704303724, 0.0005259067350094999, 0.0005857729519814698, 0.0005892809652445365, 0.0007318888048974992, 0.0006231819635688035, 0.0011590847610009473, 0.0004714832688097967, 0.0004621817905858938, 0.0008953458181262683, 0.0002473538190668532, 0.000634796515197507, 0.0005061946011760008, 0.0002473641513610509, 0.0012169669559503446, 0.00022938559457326112, 0.0009906139961485543, 0.0006056749400919053, 0.0003483504445271205, 0.0010275841037284082, 0.00024806631841078235, 0.0008106012203132929, 0.00045946435724843795, 0.0005465404637880735, 0.000510318093557856, 0.0004534376055391422, 0.0007152480538934469, 0.00034065895197815747, 0.001048139734332686, 0.00021877466747015993, 0.000686492978546546, 0.00036296164295497113, 0.0004607929575340405, 0.00047453576629397586, 0.0008764306566358279, 0.00037110726819929683, 0.0007000830399789917, 0.0002987883379509144, 0.0005067720585294179, 0.0005357839521576665, 0.0003801136014378393, 0.0004856544329590206, 0.0004203491631387941, 0.0007618945986797005, 0.0006078300735246001, 0.0004478503016880088, 0.0008688958035086963, 0.0003899023829048861, 0.0007570268066752868, 0.0004161352560153481, 0.0006959722733556001, 0.00023119847065206173, 0.0009646633910988249, 0.0003328219672088136, 0.0006550811309325717, 0.0006485564821858459, 0.00040142819912645464, 0.0006565710854605396, 0.0005670260254001773, 0.0004332025562336585, 0.000560841678620886, 0.0006031507874644403, 0.0006476356395965081, 0.0005265461244453579, 0.00038670806202050574, 0.00046061325832200584, 0.0005115275670871584, 0.0004899628754051875, 0.00045344521597361385, 0.0003506318082226746, 0.00042263567169655616, 0.000679064735791076, 0.00042232357568468954, 0.0005829593396064506, 0.0007501679703718357, 0.0002249778355228534, 0.0010790303227866985, 0.00026243696412342645, 0.0011139918785931459, 0.00021137529299959345, 0.0008975470442074671, 0.00020156191249722058, 0.00106248768817967, 0.00036330145450114317, 0.0009769484098416878, 0.00021516522562040934, 0.0014487016362263196, 0.00028749584931004517, 0.0009362137122357737, 0.00037859101717207415, 0.0006851844807544639, 0.0006999344792592325, 0.0007208722440261783, 0.00028234254786007994, 0.000783316958730401, 0.0004121149688569912, 0.001211544298983888, 0.00044963205122013593, 0.0004665443800570471, 0.00048198978374920675, 0.0006331536891077881, 0.000392657924016048, 0.00041727412234880587, 0.0004513950804734964, 0.000727906804914072, 0.0005157072229116266, 0.00032642923352836786, 0.0007514652419043009, 0.0004470952449422052, 0.000527615440008578, 0.0007635822107862515, 0.0005021761753596365, 0.00048466030258073736, 0.0003323437921431805, 0.0008672395314493063, 0.0006249314591066161, 0.0005807114561755599, 0.0006707135871150045, 0.0004031550762097615, 0.0008812192132446303, 0.0003551066946064525, 0.0004914459127779884, 0.00043064235308235373, 0.000564530723790791, 0.0007212853828903788, 0.00031692798748098093, 0.0009693590691313148, 0.0003156360536778985, 0.0007420137060681053, 0.0004828838846965957, 0.00033508319409439037, 0.0008538770593647192, 0.0002836942205330663, 0.0006867023940726337, 0.0005916655280821915, 0.0005087254164087128, 0.0006763385478476647, 0.00047677287348865796, 0.0005809835431082591, 0.0003945927219048365, 0.0003722063426463517, 0.00042828892526189003, 0.0006478989710544806, 0.00037300444960441275, 0.0009112905853414046, 0.0005222039823698353, 0.00043996660353323974, 0.000736809506796912, 0.0006508100777864456, 0.00028614185180570655, 0.0010508677466953202, 0.00035731051005979085, 0.0007204846383433844, 0.00031325518137969964, 0.0005217856882181742, 0.0003940177036261659, 0.0008152685998660971, 0.0004584985296936956, 0.0006892108879705418, 0.0002358272122636215, 0.0006398292529438414, 0.0004949600291577404, 0.0002269944397180077, 0.001043090935615795, 0.00020082063858706587, 0.0009173124788829418, 0.00033958444420470674, 0.0006785129984048431, 0.0005447316624627295, 0.0003559494859351318, 0.0007473588217425146, 0.00022532462482097377, 0.000584729805540071, 0.0005027074827256598, 0.00035490276363205666, 0.0004158989103239919, 0.0005046878732852082, 0.000484856496280087, 0.0006141973991266716, 0.0004442864955131854, 0.0006005237158735408, 0.00046195393191900716, 0.0005715770600016103, 0.0003410660919781997, 0.0004298863828237822, 0.0008226931720638453, 0.00046767213524305333, 0.0006816579096728185, 0.0004560980770333823, 0.0005335742407895402, 0.0003411062457077483, 0.00048602949271895994, 0.0005341805535048913, 0.0007601699256685688, 0.00041408979971615124, 0.00041108461289749993, 0.0008268954802248905, 0.00042749154459643387, 0.0005123014118758711, 0.00047452643308649534, 0.0005776699277947642, 0.0004978937109056582, 0.000615793692969731, 0.000321231910580102, 0.0006452251378613621, 0.000794589826176916, 0.00034366021890179326, 0.0008054081282800814, 0.0004229823256835842, 0.0003209056635908505, 0.000541470778077396, 0.0004720699664121911, 0.00046631060505925275, 0.0008963986122702707, 0.0002626026978406972, 0.0007246489099699503, 0.00034324515527531284, 0.0005749582396403178, 0.0006791242926534432, 0.0002006975213481016, 0.0008793051351111994, 0.00020836301451138874]\n",
      "[0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.2537313401699066, 0.2985074520111084, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.28358209133148193, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.26865673065185547, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.31343284249305725, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.31343284249305725, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.26865673065185547, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.26865673065185547, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.31343284249305725, 0.31343284249305725, 0.20895522832870483, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.2238806039094925, 0.31343284249305725, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.2238806039094925, 0.23880596458911896, 0.23880596458911896, 0.26865673065185547, 0.23880596458911896]\n"
     ]
    }
   ],
   "source": [
    "regressor = Sequential()\n",
    "regressor.add(Bidirectional(LSTM(units=30, return_sequences=True), input_shape = (x_train.shape[1],1) ) )\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units= 30 , return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units= 30 , return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(LSTM(units= 30))\n",
    "regressor.add(Dropout(0.2))\n",
    "regressor.add(Dense(units = n_future,activation='linear'))\n",
    "regressor.compile(optimizer='adam', loss='mean_squared_error',metrics=['acc'])\n",
    "history = regressor.fit(x_train, y_train, epochs=500,batch_size=32,validation_split=0.20 )\n",
    "\n",
    "print(history.history['loss'])\n",
    "print(history.history['acc'])\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test dataset\n",
    "testdataset = pd.read_csv('Chennai_avg_temp (3).csv')\n",
    "#get only the temperature column\n",
    "testdataset = testdataset.iloc[0:30,3:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Avg. Temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>71.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>71.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>72.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>68.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>69.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>73.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>73.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>70.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>74.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>74.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>71.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>71.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>73.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>71.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>71.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>70.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>73.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>77.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>76.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>76.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Avg. Temp\n",
       "0        71.8\n",
       "1        72.0\n",
       "2        70.3\n",
       "3        69.7\n",
       "4        71.3\n",
       "5        73.9\n",
       "6        72.9\n",
       "7        70.2\n",
       "8        68.4\n",
       "9        68.0\n",
       "10       69.9\n",
       "11       69.8\n",
       "12       71.0\n",
       "13       73.9\n",
       "14       73.4\n",
       "15       70.4\n",
       "16       74.9\n",
       "17       74.5\n",
       "18       73.0\n",
       "19       71.9\n",
       "20       71.3\n",
       "21       73.3\n",
       "22       71.4\n",
       "23       71.3\n",
       "24       70.2\n",
       "25       73.0\n",
       "26       73.8\n",
       "27       77.5\n",
       "28       76.8\n",
       "29       76.1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_temperature = pd.read_csv('Chennai_avg_temp (3).csv')\n",
    "real_temperature = real_temperature.iloc[30:34,3:4]\n",
    "\n",
    "real_temperature = real_temperature.values.reshape(len(real_temperature), 1)\n",
    "testing = sc.transform(testdataset)\n",
    "testing = np.array(testing)\n",
    "testing = np.reshape(testing,(testing.shape[1],testing.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78.3],\n",
       "       [78.9],\n",
       "       [78.3],\n",
       "       [73.3]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_temperature = regressor.predict(testing)\n",
    "predicted_temperature = sc.inverse_transform(predicted_temperature)\n",
    "predicted_temperature = np.reshape(predicted_temperature,(predicted_temperature.shape[1],predicted_temperature.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[78.18913],\n",
       "       [78.36298],\n",
       "       [77.81377],\n",
       "       [78.12587]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained model vs validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])\n",
    "pyplot.title('model train vs validation loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'validation'], loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RT = real_temperature\n",
    "PT = predicted_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.95652999])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_MSE= (    (   ((PT[0]-RT[0])**2) + ((PT[1]-RT[1])**2) + ((PT[2]-RT[2])**2) + ((PT[3]-RT[3])**2)    )/4 ) \n",
    "\n",
    "model_MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean Square Errors(MSE) is a measure of accuracy of the model. Lower the MSE, more is the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
